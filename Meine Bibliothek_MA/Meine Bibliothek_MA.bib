
@misc{chan_affine_2024,
	title = {On {Affine} {Homotopy} between {Language} {Encoders}},
	url = {http://arxiv.org/abs/2406.02329},
	urldate = {2025-01-06},
	publisher = {arXiv},
	author = {Chan, Robin S.M. and Boumasmoud, Reda and Svete, Anej and Ren, Yuxin and Guo, Qipeng and Jin, Zhijing and Ravfogel, Shauli and Sachan, Mrinmaya and Schölkopf, Bernhard and El-Assady, Mennatallah and Cotterell, Ryan},
	month = dec,
	year = {2024},
	doi = {10.48550/arXiv.2406.02329},
	note = {Issue: arXiv:2406.02329
arXiv: 2406.02329 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@book{pedrycz_deep_2020,
	address = {Cham},
	series = {Studies in {Computational} {Intelligence}},
	title = {Deep {Learning}: {Concepts} and {Architectures}},
	volume = {866},
	isbn = {978-3-030-31755-3 978-3-030-31756-0},
	shorttitle = {Deep {Learning}},
	url = {http://link.springer.com/10.1007/978-3-030-31756-0},
	urldate = {2025-01-09},
	publisher = {Springer International Publishing},
	author = {Pedrycz, Witold and Chen, Shyi-Ming},
	editor = {Pedrycz, Witold and Chen, Shyi-Ming},
	year = {2020},
	doi = {10.1007/978-3-030-31756-0},
}

@inproceedings{kornblith_similarity_2019,
	title = {Similarity of {Neural} {Network} {Representations} {Revisited}},
	url = {https://proceedings.mlr.press/v97/kornblith19a.html},
	urldate = {2025-01-06},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Kornblith, Simon and Norouzi, Mohammad and Lee, Honglak and Hinton, Geoffrey},
	month = may,
	year = {2019},
	pages = {3519--3529},
	annote = {ISSN: 2640-3498},
}

@misc{klabunde_similarity_2024,
	title = {Similarity of {Neural} {Network} {Models}: {A} {Survey} of {Functional} and {Representational} {Measures}},
	shorttitle = {Similarity of {Neural} {Network} {Models}},
	url = {http://arxiv.org/abs/2305.06329},
	urldate = {2025-01-08},
	publisher = {arXiv},
	author = {Klabunde, Max and Schumacher, Tobias and Strohmaier, Markus and Lemmerich, Florian},
	month = aug,
	year = {2024},
	doi = {10.48550/arXiv.2305.06329},
	note = {Issue: arXiv:2305.06329
arXiv: 2305.06329 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{sellam_multiberts_2022,
	title = {The {MultiBERTs}: {BERT} {Reproductions} for {Robustness} {Analysis}},
	url = {https://arxiv.org/abs/2106.16163},
	author = {Sellam, Thibault and Yadlowsky, Steve and Wei, Jason and Saphra, Naomi and D'Amour, Alexander and Linzen, Tal and Bastings, Jasmijn and Turc, Iulia and Eisenstein, Jacob and Das, Dipanjan and Tenney, Ian and Pavlick, Ellie},
	year = {2022},
	note = {\_eprint: 2106.16163},
}

@article{clark_electra_2020,
	title = {{ELECTRA}: {PRE}-{TRAINING} {TEXT} {ENCODERS} {AS} {DISCRIMINATORS} {RATHER} {THAN} {GENERATORS}},
	author = {Clark, Kevin and Luong, Minh-Thang and Le, Quoc V},
	year = {2020},
}

@article{liu_roberta_2019,
	title = {{RoBERTa}: {A} {Robustly} {Optimized} {BERT} {Pretraining} {Approach}},
	shorttitle = {{RoBERTa}},
	url = {https://www.semanticscholar.org/paper/RoBERTa%3A-A-Robustly-Optimized-BERT-Pretraining-Liu-Ott/077f8329a7b6fa3b7c877a57b81eb6c18b5f87de},
	urldate = {2025-01-12},
	journal = {ArXiv},
	author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, M. and Zettlemoyer, Luke and Stoyanov, Veselin},
	month = jul,
	year = {2019},
}

@misc{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	url = {https://aclanthology.org/N19-1423/},
	journal = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina and Burstein, Jill and Doran, Christy and Solorio, Thamar},
	month = jun,
	year = {2019},
	doi = {10.18653/v1/N19-1423},
	note = {Pages: 4171–4186
Place: Minneapolis, Minnesota
Publisher: Association for Computational Linguistics},
}

@inproceedings{socher_recursive_2013,
	address = {Seattle, Washington, USA},
	title = {Recursive {Deep} {Models} for {Semantic} {Compositionality} {Over} a {Sentiment} {Treebank}},
	url = {https://aclanthology.org/D13-1170/},
	urldate = {2025-01-14},
	booktitle = {Proceedings of the 2013 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Socher, Richard and Perelygin, Alex and Wu, Jean and Chuang, Jason and Manning, Christopher D. and Ng, Andrew and Potts, Christopher},
	editor = {Yarowsky, David and Baldwin, Timothy and Korhonen, Anna and Livescu, Karen and Bethard, Steven},
	month = oct,
	year = {2013},
	pages = {1631--1642},
}

@inproceedings{dolan_automatically_2005,
	title = {Automatically {Constructing} a {Corpus} of {Sentential} {Paraphrases}},
	url = {https://aclanthology.org/I05-5002/},
	urldate = {2025-01-14},
	booktitle = {Proceedings of the {Third} {International} {Workshop} on {Paraphrasing} ({IWP2005})},
	author = {Dolan, William B. and Brockett, Chris},
	year = {2005},
	file = {1682_electra_pre_training_text_enco:files/156/1682_electra_pre_training_text_enco.pdf:application/pdf;PDF:files/90/Dolan und Brockett - 2005 - Automatically Constructing a Corpus of Sentential Paraphrases.pdf:application/pdf},
}

@book{antiga_deep_2020,
	address = {Shelter Island, NY},
	title = {Deep learning with {PyTorch}},
	isbn = {978-1-61729-526-3 978-1-63835-407-9},
	publisher = {Manning},
	author = {Antiga, Luca and Stevens, Eli and Viehmann, Thomas},
	year = {2020},
}

@misc{wang_glue_2019,
	title = {{GLUE}: {A} {Multi}-{Task} {Benchmark} and {Analysis} {Platform} for {Natural} {Language} {Understanding}},
	shorttitle = {{GLUE}},
	url = {http://arxiv.org/abs/1804.07461},
	urldate = {2025-01-23},
	publisher = {arXiv},
	author = {Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
	month = feb,
	year = {2019},
	doi = {10.48550/arXiv.1804.07461},
	note = {Issue: arXiv:1804.07461
arXiv: 1804.07461 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{krantz_what_2024,
	title = {What {Is} {Data} {Poisoning}? {\textbackslash}textbar {IBM}},
	shorttitle = {What {Is} {Data} {Poisoning}?},
	url = {https://www.ibm.com/think/topics/data-poisoning},
	abstract = {Data poisoning occurs when threat actors manipulate or corrupt the training data used to develop artificial intelligence (AI) and machine learning (ML) models.},
	urldate = {2025-01-24},
	author = {Krantz, Tom and Jonker, Alexandra},
	month = dec,
	year = {2024},
	file = {PDF:files/96/Krantz und Jonker - 2024 - What Is Data Poisoning textbar IBM.pdf:application/pdf},
}

@misc{cotterell_formal_2024,
	title = {Formal {Aspects} of {Language} {Modeling}},
	url = {http://arxiv.org/abs/2311.04329},
	urldate = {2025-01-11},
	publisher = {arXiv},
	author = {Cotterell, Ryan and Svete, Anej and Meister, Clara and Liu, Tianyu and Du, Li},
	month = apr,
	year = {2024},
	doi = {10.48550/arXiv.2311.04329},
	note = {Issue: arXiv:2311.04329
arXiv: 2311.04329 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@book{knudson_toolkit_2024,
	address = {Berlin, Boston},
	title = {A {Toolkit}},
	isbn = {978-3-11-101485-2},
	url = {https://doi.org/10.1515/9783111014852},
	urldate = {2025-02-10},
	publisher = {De Gruyter},
	author = {Knudson, Kevin P.},
	year = {2024},
	doi = {doi:10.1515/9783111014852},
}

@inproceedings{ding_grounding_2021,
	title = {Grounding {Representation} {Similarity} {Through} {Statistical} {Testing}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/hash/0c0bf917c7942b5a08df71f9da626f97-Abstract.html},
	urldate = {2025-02-11},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Ding, Frances and Denain, Jean-Stanislas and Steinhardt, Jacob},
	year = {2021},
	pages = {1556--1568},
}

@article{shahbazi_using_2021,
	title = {Using distance on the {Riemannian} manifold to compare representations in brain and in models},
	volume = {239},
	issn = {1053-8119},
	url = {https://www.sciencedirect.com/science/article/pii/S1053811921005474},
	doi = {https://doi.org/10.1016/j.neuroimage.2021.118271},
	journal = {NeuroImage},
	author = {Shahbazi, Mahdiyar and Shirali, Ali and Aghajan, Hamid and Nili, Hamed},
	year = {2021},
	pages = {118271},
}

@article{boix-adsera_gulp_nodate,
	title = {{GULP}: a prediction-based metric between representations},
	author = {Boix-Adserà, Enric and Lawrence, Hannah and Stepaniants, George and Rigollet, Philippe},
}

@inproceedings{williams_generalized_2021,
	title = {Generalized {Shape} {Metrics} on {Neural} {Representations}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/hash/252a3dbaeb32e7690242ad3b556e626b-Abstract.html},
	urldate = {2025-02-11},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Williams, Alex H and Kunz, Erin and Kornblith, Simon and Linderman, Scott},
	year = {2021},
	pages = {4738--4750},
}

@book{small_statistical_2012,
	series = {Springer {Series} in {Statistics}},
	title = {The {Statistical} {Theory} of {Shape}},
	isbn = {978-1-4612-4032-7},
	url = {https://books.google.ch/books?id=C43bBwAAQBAJ},
	publisher = {Springer New York},
	author = {Small, C.G.},
	year = {2012},
	lccn = {96013587},
}

@misc{wei_fang_survey_nodate,
	title = {Survey on distance metric learning and dimensionality reduction in data mining {\textbackslash}textbar {Data} {Mining} and {Knowledge} {Discovery}},
	url = {https://link.springer.com/article/10.1007/s10618-014-0356-z},
	urldate = {2025-02-11},
	author = {Wei Fang, Sun Jimeng},
}

@article{dasgupta_performance_2005,
	series = {Special {Issue} on {COLT} 2002},
	title = {Performance guarantees for hierarchical clustering},
	volume = {70},
	issn = {0022-0000},
	url = {https://www.sciencedirect.com/science/article/pii/S0022000004001321},
	doi = {10.1016/j.jcss.2004.10.006},
	number = {4},
	urldate = {2025-02-11},
	journal = {Journal of Computer and System Sciences},
	author = {Dasgupta, Sanjoy and Long, Philip M.},
	month = jun,
	year = {2005},
	keywords = {-Center, Complete linkage, Hierarchical clustering},
	pages = {555--569},
}

@misc{chang_mathematical_2016,
	title = {A {Mathematical} {Theory} for {Clustering} in {Metric} {Spaces}},
	url = {http://dx.doi.org/10.1109/TNSE.2016.2516339},
	journal = {IEEE Transactions on Network Science and Engineering},
	author = {Chang, Cheng-Shang and Liao, Wanjiun and Chen, Yu-Sheng and Liou, Li-Heng},
	month = jan,
	year = {2016},
	doi = {10.1109/tnse.2016.2516339},
	note = {ISSN: 2327-4697
Issue: 1
Pages: 2–16
Publisher: Institute of Electrical and Electronics Engineers (IEEE)
Volume: 3},
}

@inproceedings{baraty_impact_2011,
	address = {Berlin, Heidelberg},
	title = {The {Impact} of {Triangular} {Inequality} {Violations} on {Medoid}-{Based} {Clustering}},
	isbn = {978-3-642-21916-0},
	doi = {10.1007/978-3-642-21916-0_31},
	booktitle = {Foundations of {Intelligent} {Systems}},
	publisher = {Springer},
	author = {Baraty, Saaid and Simovici, Dan A. and Zara, Catalin},
	editor = {Kryszkiewicz, Marzena and Rybinski, Henryk and Skowron, Andrzej and Raś, Zbigniew W.},
	year = {2011},
	pages = {280--289},
}

@book{zhou_machine_2021,
	address = {Singapore},
	title = {Machine {Learning}},
	isbn = {978-981-15-1966-6 978-981-15-1967-3},
	url = {https://link.springer.com/10.1007/978-981-15-1967-3},
	urldate = {2025-02-13},
	publisher = {Springer Singapore},
	author = {Zhou, Zhi-Hua},
	year = {2021},
	doi = {10.1007/978-981-15-1967-3},
}

@book{jung_machine_2022,
	series = {Machine {Learning}: {Foundations}, {Methodologies}, and {Applications}},
	title = {Machine {Learning}: {The} {Basics}},
	isbn = {978-981-16-8192-9},
	url = {https://books.google.de/books?id=Sr62zgEACAAJ},
	publisher = {Springer Nature Singapore},
	author = {Jung, A.},
	year = {2022},
}

@incollection{han_2_2012,
	address = {Boston},
	edition = {Third Edition},
	series = {The {Morgan} {Kaufmann} {Series} in {Data} {Management} {Systems}},
	title = {2 - {Getting} to {Know} {Your} {Data}},
	isbn = {978-0-12-381479-1},
	booktitle = {Data {Mining} ({Third} {Edition})},
	publisher = {Morgan Kaufmann},
	author = {Han, Jiawei and Kamber, Micheline and Pei, Jian},
	editor = {Han, Jiawei and Kamber, Micheline and Pei, Jian},
	year = {2012},
	doi = {10.1016/B978-0-12-381479-1.00002-2},
	pages = {39--82},
}

@article{garcia_comprehensive_2015,
	title = {A comprehensive survey on safe reinforcement learning},
	volume = {16},
	number = {1},
	journal = {Journal of Machine Learning Research},
	author = {Garcıa, Javier and Fernández, Fernando},
	year = {2015},
	pages = {1437--1480},
}

@book{haralambous_course_2024,
	address = {Cham},
	title = {A {Course} in {Natural} {Language} {Processing}},
	isbn = {978-3-031-27225-7 978-3-031-27226-4},
	url = {https://link.springer.com/10.1007/978-3-031-27226-4},
	urldate = {2025-02-13},
	publisher = {Springer International Publishing},
	author = {Haralambous, Yannis},
	year = {2024},
	doi = {10.1007/978-3-031-27226-4},
}

@book{kamath_deep_2019,
	address = {Cham},
	title = {Deep {Learning} for {NLP} and {Speech} {Recognition}},
	isbn = {978-3-030-14595-8 978-3-030-14596-5},
	url = {http://link.springer.com/10.1007/978-3-030-14596-5},
	urldate = {2025-02-13},
	publisher = {Springer International Publishing},
	author = {Kamath, Uday and Liu, John and Whitaker, James},
	year = {2019},
	doi = {10.1007/978-3-030-14596-5},
}

@book{goyal_deep_2018,
	address = {Berkeley, CA},
	title = {Deep {Learning} for {Natural} {Language} {Processing}},
	isbn = {978-1-4842-3684-0 978-1-4842-3685-7},
	url = {http://link.springer.com/10.1007/978-1-4842-3685-7},
	urldate = {2025-02-14},
	publisher = {Apress},
	author = {Goyal, Palash and Pandey, Sumit and Jain, Karan},
	year = {2018},
	doi = {10.1007/978-1-4842-3685-7},
}

@misc{vaswani_attention_2023,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	urldate = {2025-02-16},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = aug,
	year = {2023},
	doi = {10.48550/arXiv.1706.03762},
	note = {Issue: arXiv:1706.03762
arXiv: 1706.03762 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{otter_survey_2021,
	title = {A {Survey} of the {Usages} of {Deep} {Learning} for {Natural} {Language} {Processing}},
	volume = {32},
	issn = {2162-2388},
	url = {https://ieeexplore.ieee.org/document/9075398/?arnumber=9075398},
	doi = {10.1109/TNNLS.2020.2979670},
	number = {2},
	urldate = {2025-02-13},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Otter, Daniel W. and Medina, Julian R. and Kalita, Jugal K.},
	month = feb,
	year = {2021},
	keywords = {Computational linguistics, Computer architecture, Decoding, deep learning, machine learning, Machine learning, Natural language processing, natural language processing (NLP), neural networks, Neural networks, Training},
	pages = {604--624},
	annote = {Conference Name: IEEE Transactions on Neural Networks and Learning Systems},
}

@book{dieck_mengentheoretische_2009,
	title = {Mengentheoretische {Topologie}},
	author = {Dieck, tom Tammo},
	year = {2009},
}

@article{lawvere_metric_1973,
	title = {Metric spaces, generalized logic, and closed categories},
	volume = {43},
	issn = {0370-7377},
	url = {http://link.springer.com/10.1007/BF02924844},
	doi = {10.1007/BF02924844},
	number = {1},
	urldate = {2025-02-18},
	journal = {Rendiconti del Seminario Matematico e Fisico di Milano},
	author = {Lawvere, F. William},
	month = dec,
	year = {1973},
	pages = {135--166},
	file = {PDF:files/66/Lawvere - 1973 - Metric spaces, generalized logic, and closed categories.pdf:application/pdf},
}

@misc{zhang_introduction_2023,
	title = {An {Introduction} to {Bi}-level {Optimization}: {Foundations} and {Applications} in {Signal} {Processing} and {Machine} {Learning}},
	shorttitle = {An {Introduction} to {Bi}-level {Optimization}},
	url = {http://arxiv.org/abs/2308.00788},
	doi = {10.48550/arXiv.2308.00788},
	abstract = {Recently, bi-level optimization (BLO) has taken center stage in some very exciting developments in the area of signal processing (SP) and machine learning (ML). Roughly speaking, BLO is a classical optimization problem that involves two levels of hierarchy (i.e., upper and lower levels), wherein obtaining the solution to the upper-level problem requires solving the lower-level one. BLO has become popular largely because it is powerful in modeling problems in SP and ML, among others, that involve optimizing nested objective functions. Prominent applications of BLO range from resource allocation for wireless systems to adversarial machine learning. In this work, we focus on a class of tractable BLO problems that often appear in SP and ML applications. We provide an overview of some basic concepts of this class of BLO problems, such as their optimality conditions, standard algorithms (including their optimization principles and practical implementations), as well as how they can be leveraged to obtain stateof-the-art results for a number of key SP and ML applications. Further, we discuss some recent advances in BLO theory, its implications for applications, and point out some limitations of the state-of-the-art that require significant future research efforts. Overall, we hope that this article can serve to accelerate the adoption of BLO as a generic tool to model, analyze, and innovate on a wide array of emerging SP and ML applications.},
	language = {en},
	urldate = {2025-05-04},
	publisher = {arXiv},
	author = {Zhang, Yihua and Khanduri, Prashant and Tsaknakis, Ioannis and Yao, Yuguang and Hong, Mingyi and Liu, Sijia},
	month = dec,
	year = {2023},
	note = {arXiv:2308.00788 [cs]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control},
	file = {PDF:files/38/Zhang et al. - 2023 - An Introduction to Bi-level Optimization Foundations and Applications in Signal Processing and Mach.pdf:application/pdf},
}

@article{liu_bilevel_2024,
	title = {Bilevel optimization for automated machine learning: a new perspective on framework and algorithm},
	volume = {11},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2095-5138, 2053-714X},
	shorttitle = {Bilevel optimization for automated machine learning},
	url = {https://academic.oup.com/nsr/article/doi/10.1093/nsr/nwad292/7440017},
	doi = {10.1093/nsr/nwad292},
	abstract = {Formulating the methodology of machine learning by bilevel optimization techniques provides a new perspective to understand and solve automated machine learning problems.},
	language = {en},
	number = {8},
	urldate = {2025-05-04},
	journal = {National Science Review},
	author = {Liu, Risheng and Lin, Zhouchen},
	month = jul,
	year = {2024},
	pages = {nwad292},
	file = {PDF:files/40/Liu und Lin - 2024 - Bilevel optimization for automated machine learning a new perspective on framework and algorithm.pdf:application/pdf},
}

@inproceedings{qu_comprehensive_2021,
	address = {Qingdao, China},
	title = {A {Comprehensive} {Review} of {Machine} {Learning} in {Multi}-objective {Optimization}},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	isbn = {978-1-6654-1270-4},
	url = {https://ieeexplore.ieee.org/document/9515233/},
	doi = {10.1109/BDAI52447.2021.9515233},
	abstract = {In the real world, it is challenging to calculate a trade-off alternative with traditional classical methods for complex non-linear systems, which always involve multiple conflicting objectives. Such complicated systems urgently desire advanced methods to conquer the multi-objective optimization problems (MOPs). As a promising AI method, the development and application of Machine Learning (ML) attract increasingly more attention from researchers. The natures of ML methods, such as parallel computation possibility, no need for any priori assumptions, etc., ensure the effectiveness and efficiency for solving MOPs. However, as we know, there is no literature related to the comprehensive review of ML in multi-objective optimization domain until now. This literature review aims to provide researchers a global view of mainstream ML methods for MOO in a general domain and a reference for applying ML methods to solve a specific type of MOPs. In this paper, the general ML mainstream methods are summarized, based on which the literature relating to ML on MOPs are retrieved in comprehensive domains. The relevant literature is categorized according to the emphasis of object types, purposes and methods, and the categorization results are finally analyzed and discussed.},
	language = {en},
	urldate = {2025-05-04},
	booktitle = {2021 {IEEE} 4th {International} {Conference} on {Big} {Data} and {Artificial} {Intelligence} ({BDAI})},
	publisher = {IEEE},
	author = {Qu, Qu and Ma, Zheng and Clausen, Anders and Jorgensen, Bo Norregaard},
	month = jul,
	year = {2021},
	pages = {7--14},
	file = {PDF:files/42/Qu et al. - 2021 - A Comprehensive Review of Machine Learning in Multi-objective Optimization.pdf:application/pdf},
}

@misc{vaswani_attention_2023-1,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	doi = {10.48550/arXiv.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	language = {en},
	urldate = {2025-05-04},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = aug,
	year = {2023},
	note = {arXiv:1706.03762 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: 15 pages, 5 figures},
	file = {PDF:files/44/Vaswani et al. - 2023 - Attention Is All You Need.pdf:application/pdf},
}

@misc{kumar_equivalent_2019,
	title = {Equivalent and {Approximate} {Transformations} of {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1905.11428},
	doi = {10.48550/arXiv.1905.11428},
	abstract = {Two networks are equivalent if they produce the same output for any given input. In this paper, we study the possibility of transforming a deep neural network to another network with a different number of units or layers, which can be either equivalent, a local exact approximation, or a global linear approximation of the original network. On the practical side, we show that certain rectified linear units (ReLUs) can be safely removed from a network if they are always active or inactive for any valid input. If we only need an equivalent network for a smaller domain, then more units can be removed and some layers collapsed. On the theoretical side, we constructively show that for any feed-forward ReLU network, there exists a global linear approximation to a 2-hidden-layer shallow network with a fixed number of units. This result is a balance between the increasing number of units for arbitrary approximation with a single layer and the known upper bound of \${\textbackslash}lceil log(n\_0+1){\textbackslash}rceil +1\$ layers for exact representation, where \$n\_0\$ is the input dimension. While the transformed network may require an exponential number of units to capture the activation patterns of the original network, we show that it can be made substantially smaller by only accounting for the patterns that define linear regions. Based on experiments with ReLU networks on the MNIST dataset, we found that \$l\_1\$-regularization and adversarial training reduces the number of linear regions significantly as the number of stable units increases due to weight sparsity. Therefore, we can also intentionally train ReLU networks to allow for effective loss-less compression and approximation.},
	urldate = {2025-05-04},
	publisher = {arXiv},
	author = {Kumar, Abhinav and Serra, Thiago and Ramalingam, Srikumar},
	month = may,
	year = {2019},
	note = {arXiv:1905.11428 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
	file = {Full Text PDF:files/49/Kumar et al. - 2019 - Equivalent and Approximate Transformations of Deep Neural Networks.pdf:application/pdf;Snapshot:files/48/1905.html:text/html},
}

@article{wood_estimation_1996,
	title = {Estimation of the {Lipschitz} constant of a function},
	volume = {8},
	copyright = {http://www.springer.com/tdm},
	issn = {0925-5001, 1573-2916},
	url = {http://link.springer.com/10.1007/BF00229304},
	doi = {10.1007/BF00229304},
	abstract = {A number of global optimisation algorithms rely on the value of the Lipschitz constant of the objective function. In this paper we present a stochastic method for estimating the Lipschitz constant. We show that the largest slope in a fixed size sample of slopes has an approximate Reverse Weibull distribution. Such a distribution is fitted to the largest slopes and the location parameter used as an estimator of the Lipschitz constant. Numerical results are presented.},
	language = {en},
	number = {1},
	urldate = {2025-05-07},
	journal = {Journal of Global Optimization},
	author = {Wood, G.R. and Zhang, B.P.},
	month = jan,
	year = {1996},
	file = {PDF:files/50/Wood und Zhang - 1996 - Estimation of the Lipschitz constant of a function.pdf:application/pdf},
}

@misc{zhang_introduction_2023-1,
	title = {An {Introduction} to {Bi}-level {Optimization}: {Foundations} and {Applications} in {Signal} {Processing} and {Machine} {Learning}},
	shorttitle = {An {Introduction} to {Bi}-level {Optimization}},
	url = {http://arxiv.org/abs/2308.00788},
	doi = {10.48550/arXiv.2308.00788},
	abstract = {Recently, bi-level optimization (BLO) has taken center stage in some very exciting developments in the area of signal processing (SP) and machine learning (ML). Roughly speaking, BLO is a classical optimization problem that involves two levels of hierarchy (i.e., upper and lower levels), wherein obtaining the solution to the upper-level problem requires solving the lower-level one. BLO has become popular largely because it is powerful in modeling problems in SP and ML, among others, that involve optimizing nested objective functions. Prominent applications of BLO range from resource allocation for wireless systems to adversarial machine learning. In this work, we focus on a class of tractable BLO problems that often appear in SP and ML applications. We provide an overview of some basic concepts of this class of BLO problems, such as their optimality conditions, standard algorithms (including their optimization principles and practical implementations), as well as how they can be leveraged to obtain stateof-the-art results for a number of key SP and ML applications. Further, we discuss some recent advances in BLO theory, its implications for applications, and point out some limitations of the state-of-the-art that require significant future research efforts. Overall, we hope that this article can serve to accelerate the adoption of BLO as a generic tool to model, analyze, and innovate on a wide array of emerging SP applications.},
	language = {en},
	urldate = {2025-05-07},
	publisher = {arXiv},
	author = {Zhang, Yihua and Khanduri, Prashant and Tsaknakis, Ioannis and Yao, Yuguang and Hong, Mingyi and Liu, Sijia},
	month = dec,
	year = {2023},
	note = {arXiv:2308.00788 [cs]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control},
	file = {PDF:files/52/Zhang et al. - 2023 - An Introduction to Bi-level Optimization Foundations and Applications in Signal Processing and Mach.pdf:application/pdf},
}

@article{colson_overview_2007,
	title = {An overview of bilevel optimization},
	volume = {153},
	issn = {0254-5330, 1572-9338},
	url = {https://link.springer.com/10.1007/s10479-007-0176-2},
	doi = {10.1007/s10479-007-0176-2},
	abstract = {This paper is devoted to bilevel optimization, a branch of mathematical programming of both practical and theoretical interest. Starting with a simple example, we proceed towards a general formulation. We then present ﬁelds of application, focus on solution approaches, and make the connection with MPECs (Mathematical Programs with Equilibrium Constraints).},
	language = {en},
	number = {1},
	urldate = {2025-05-07},
	journal = {Annals of Operations Research},
	author = {Colson, Benoît and Marcotte, Patrice and Savard, Gilles},
	month = sep,
	year = {2007},
	pages = {235--256},
	file = {PDF:files/54/Colson et al. - 2007 - An overview of bilevel optimization.pdf:application/pdf},
}

@misc{beck_computationally_2023,
	title = {On a {Computationally} {Ill}-{Behaved} {Bilevel} {Problem} with a {Continuous} and {Nonconvex} {Lower} {Level}},
	url = {http://arxiv.org/abs/2202.01033},
	doi = {10.48550/arXiv.2202.01033},
	abstract = {It is well known that bilevel optimization problems are hard to solve both in theory and practice. In this paper, we highlight a further computational diﬃculty when it comes to solving bilevel problems with continuous but nonconvex lower levels. Even if the lower-level problem is solved to ε-feasibility regarding its nonlinear constraints for an arbitrarily small but positive ε, the obtained bilevel solution as well as its objective value may be arbitrarily far away from the actual bilevel solution and its actual objective value. This result even holds for bilevel problems for which the nonconvex lower level is uniquely solvable, for which the strict complementarity condition holds, for which the feasible set is convex, and for which Slater’s constraint qualiﬁcation is satisﬁed for all feasible upper-level decisions. Since the consideration of ε-feasibility cannot be avoided when solving nonconvex problems to global optimality, our result shows that computational bilevel optimization with continuous and nonconvex lower levels needs to be done with great care. Finally, we illustrate that the nonlinearities in the lower level are the key reason for the observed bad behavior by showing that linear bilevel problems behave much better at least on the level of feasible solutions.},
	language = {en},
	urldate = {2025-05-07},
	publisher = {arXiv},
	author = {Beck, Yasmine and Bienstock, Daniel and Schmidt, Martin and Thürauf, Johannes},
	month = may,
	year = {2023},
	note = {arXiv:2202.01033 [math]},
	keywords = {Mathematics - Optimization and Control},
	file = {PDF:files/56/Beck et al. - 2023 - On a Computationally Ill-Behaved Bilevel Problem with a Continuous and Nonconvex Lower Level.pdf:application/pdf},
}

@misc{sinha_review_2020,
	title = {A {Review} on {Bilevel} {Optimization}: {From} {Classical} to {Evolutionary} {Approaches} and {Applications}},
	shorttitle = {A {Review} on {Bilevel} {Optimization}},
	url = {http://arxiv.org/abs/1705.06270},
	doi = {10.48550/arXiv.1705.06270},
	abstract = {Bilevel optimization is deﬁned as a mathematical program, where an optimization problem contains another optimization problem as a constraint. These problems have received signiﬁcant attention from the mathematical programming community. Only limited work exists on bilevel problems using evolutionary computation techniques; however, recently there has been an increasing interest due to the proliferation of practical applications and the potential of evolutionary algorithms in tackling these problems. This paper provides a comprehensive review on bilevel optimization from the basic principles to solution strategies; both classical and evolutionary. A number of potential application problems are also discussed. To offer the readers insights on the prominent developments in the ﬁeld of bilevel optimization, we have performed an automated textanalysis of an extended list of papers published on bilevel optimization to date. This paper should motivate evolutionary computation researchers to pay more attention to this practical yet challenging area.},
	language = {en},
	urldate = {2025-05-13},
	publisher = {arXiv},
	author = {Sinha, Ankur and Malo, Pekka and Deb, Kalyanmoy},
	month = dec,
	year = {2020},
	note = {arXiv:1705.06270 [math]},
	keywords = {Mathematics - Optimization and Control, Computer Science - Neural and Evolutionary Computing},
	file = {PDF:files/61/Sinha et al. - 2020 - A Review on Bilevel Optimization From Classical to Evolutionary Approaches and Applications.pdf:application/pdf},
}

@misc{geuchen_upper_2024,
	title = {Upper and lower bounds for the {Lipschitz} constant of random neural networks},
	url = {http://arxiv.org/abs/2311.01356},
	doi = {10.48550/arXiv.2311.01356},
	abstract = {Empirical studies have widely demonstrated that neural networks are highly sensitive to small, adversarial perturbations of the input. The worst-case robustness against these so-called adversarial examples can be quantified by the Lipschitz constant of the neural network. In this paper, we study upper and lower bounds for the Lipschitz constant of random ReLU neural networks. Specifically, we assume that the weights and biases follow a generalization of the He initialization, where general symmetric distributions for the biases are permitted. For shallow neural networks, we characterize the Lipschitz constant up to an absolute numerical constant. For deep networks of fixed depth and sufficiently large width, our established upper bound is larger than the lower bound by a factor that is logarithmic in the width.},
	language = {en},
	urldate = {2025-05-15},
	publisher = {arXiv},
	author = {Geuchen, Paul and Heindl, Thomas and Stöger, Dominik and Voigtlaender, Felix},
	month = jan,
	year = {2024},
	note = {arXiv:2311.01356 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Probability},
	file = {PDF:files/63/Geuchen et al. - 2024 - Upper and lower bounds for the Lipschitz constant of random neural networks.pdf:application/pdf},
}

@book{zhou_machine_2021-1,
	address = {Singapore},
	title = {Machine {Learning}},
	copyright = {https://www.springer.com/tdm},
	isbn = {978-981-15-1966-6 978-981-15-1967-3},
	url = {https://link.springer.com/10.1007/978-981-15-1967-3},
	language = {en},
	urldate = {2025-05-18},
	publisher = {Springer Singapore},
	author = {Zhou, Zhi-Hua},
	year = {2021},
	doi = {10.1007/978-981-15-1967-3},
	file = {PDF:files/68/Zhou - 2021 - Machine Learning.pdf:application/pdf},
}

@book{fernandes_de_mello_machine_2018,
	address = {Cham},
	title = {Machine {Learning}: {A} {Practical} {Approach} on the {Statistical} {Learning} {Theory}},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-3-319-94988-8 978-3-319-94989-5},
	shorttitle = {Machine {Learning}},
	url = {http://link.springer.com/10.1007/978-3-319-94989-5},
	language = {en},
	urldate = {2025-05-18},
	publisher = {Springer International Publishing},
	author = {Fernandes De Mello, Rodrigo and Antonelli Ponti, Moacir},
	year = {2018},
	doi = {10.1007/978-3-319-94989-5},
	file = {PDF:files/70/Fernandes De Mello und Antonelli Ponti - 2018 - Machine Learning A Practical Approach on the Statistical Learning Theory.pdf:application/pdf},
}

@book{zhou_machine_2021-2,
	address = {Singapore},
	title = {Machine {Learning}},
	copyright = {https://www.springer.com/tdm},
	isbn = {978-981-15-1966-6 978-981-15-1967-3},
	url = {https://link.springer.com/10.1007/978-981-15-1967-3},
	language = {en},
	urldate = {2025-05-18},
	publisher = {Springer Singapore},
	author = {Zhou, Zhi-Hua},
	year = {2021},
	doi = {10.1007/978-981-15-1967-3},
	file = {PDF:files/71/Zhou - 2021 - Machine Learning.pdf:application/pdf},
}

@book{jung_machine_2022-1,
	address = {Singapore},
	series = {Machine {Learning}: {Foundations}, {Methodologies}, and {Applications}},
	title = {Machine {Learning}: {The} {Basics}},
	copyright = {https://www.springer.com/tdm},
	isbn = {978-981-16-8192-9 978-981-16-8193-6},
	shorttitle = {Machine {Learning}},
	url = {https://link.springer.com/10.1007/978-981-16-8193-6},
	language = {en},
	urldate = {2025-05-18},
	publisher = {Springer Nature Singapore},
	author = {Jung, Alexander},
	year = {2022},
	doi = {10.1007/978-981-16-8193-6},
	file = {PDF:files/72/Jung - 2022 - Machine Learning The Basics.pdf:application/pdf},
}

@book{jo_machine_2021,
	address = {Cham},
	title = {Machine {Learning} {Foundations}: {Supervised}, {Unsupervised}, and {Advanced} {Learning}},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-3-030-65899-1 978-3-030-65900-4},
	shorttitle = {Machine {Learning} {Foundations}},
	url = {http://link.springer.com/10.1007/978-3-030-65900-4},
	language = {en},
	urldate = {2025-05-18},
	publisher = {Springer International Publishing},
	author = {Jo, Taeho},
	year = {2021},
	doi = {10.1007/978-3-030-65900-4},
	file = {PDF:files/73/Jo - 2021 - Machine Learning Foundations Supervised, Unsupervised, and Advanced Learning.pdf:application/pdf},
}

@book{haralambous_course_2024-1,
	address = {Cham},
	title = {A {Course} in {Natural} {Language} {Processing}},
	copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining},
	isbn = {978-3-031-27225-7 978-3-031-27226-4},
	url = {https://link.springer.com/10.1007/978-3-031-27226-4},
	language = {en},
	urldate = {2025-05-19},
	publisher = {Springer International Publishing},
	author = {Haralambous, Yannis},
	year = {2024},
	doi = {10.1007/978-3-031-27226-4},
	file = {PDF:files/78/Haralambous - 2024 - A Course in Natural Language Processing.pdf:application/pdf},
}

@book{deng_deep_2018,
	address = {Singapore},
	title = {Deep {Learning} in {Natural} {Language} {Processing}},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-981-10-5208-8 978-981-10-5209-5},
	url = {http://link.springer.com/10.1007/978-981-10-5209-5},
	language = {en},
	urldate = {2025-05-19},
	publisher = {Springer Singapore},
	editor = {Deng, Li and Liu, Yang},
	year = {2018},
	doi = {10.1007/978-981-10-5209-5},
	file = {PDF:files/79/Deng und Liu - 2018 - Deep Learning in Natural Language Processing.pdf:application/pdf},
}

@book{feng_formal_2023,
	address = {Singapore},
	title = {Formal {Analysis} for {Natural} {Language} {Processing}: {A} {Handbook}},
	copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining},
	isbn = {978-981-16-5171-7 978-981-16-5172-4},
	shorttitle = {Formal {Analysis} for {Natural} {Language} {Processing}},
	url = {https://link.springer.com/10.1007/978-981-16-5172-4},
	language = {en},
	urldate = {2025-05-19},
	publisher = {Springer Nature Singapore},
	author = {Feng, Zhiwei},
	year = {2023},
	doi = {10.1007/978-981-16-5172-4},
	file = {PDF:files/80/Feng - 2023 - Formal Analysis for Natural Language Processing A Handbook.pdf:application/pdf},
}

@book{deng_deep_2018-1,
	address = {Singapore},
	title = {Deep {Learning} in {Natural} {Language} {Processing}},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-981-10-5208-8 978-981-10-5209-5},
	url = {http://link.springer.com/10.1007/978-981-10-5209-5},
	language = {en},
	urldate = {2025-05-19},
	publisher = {Springer Singapore},
	editor = {Deng, Li and Liu, Yang},
	year = {2018},
	doi = {10.1007/978-981-10-5209-5},
	file = {PDF:files/84/Deng und Liu - 2018 - Deep Learning in Natural Language Processing.pdf:application/pdf},
}

@book{goyal_deep_2018-1,
	address = {Berkeley, CA},
	title = {Deep {Learning} for {Natural} {Language} {Processing}},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-1-4842-3684-0 978-1-4842-3685-7},
	url = {http://link.springer.com/10.1007/978-1-4842-3685-7},
	language = {en},
	urldate = {2025-05-19},
	publisher = {Apress},
	author = {Goyal, Palash and Pandey, Sumit and Jain, Karan},
	year = {2018},
	doi = {10.1007/978-1-4842-3685-7},
	file = {PDF:files/86/Goyal et al. - 2018 - Deep Learning for Natural Language Processing.pdf:application/pdf},
}

@misc{devlin_bert_2019-1,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	doi = {10.48550/arXiv.1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be ﬁnetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspeciﬁc architecture modiﬁcations.},
	language = {en},
	urldate = {2025-05-19},
	publisher = {arXiv},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv:1810.04805 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {PDF:files/88/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf:application/pdf},
}

@article{radford_improving_nodate,
	title = {Improving {Language} {Understanding} by {Generative} {Pre}-{Training}},
	abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classiﬁcation. Although large unlabeled text corpora are abundant, labeled data for learning these speciﬁc tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative ﬁne-tuning on each speciﬁc task. In contrast to previous approaches, we make use of task-aware input transformations during ﬁne-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures speciﬁcally crafted for each task, signiﬁcantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9\% on commonsense reasoning (Stories Cloze Test), 5.7\% on question answering (RACE), and 1.5\% on textual entailment (MultiNLI).},
	language = {en},
	author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
	file = {PDF:files/91/Radford et al. - Improving Language Understanding by Generative Pre-Training.pdf:application/pdf},
}

@article{klabunde_similarity_2025,
	title = {Similarity of {Neural} {Network} {Models}: {A} {Survey} of {Functional} and {Representational} {Measures}},
	volume = {57},
	issn = {0360-0300, 1557-7341},
	shorttitle = {Similarity of {Neural} {Network} {Models}},
	url = {http://arxiv.org/abs/2305.06329},
	doi = {10.1145/3728458},
	abstract = {Measuring similarity of neural networks to understand and improve their behavior has become an issue of great importance and research interest. In this survey, we provide a comprehensive overview of two complementary perspectives of measuring neural network similarity: (i) representational similarity, which considers how activations of intermediate layers differ, and (ii) functional similarity, which considers how models differ in their outputs. In addition to providing detailed descriptions of existing measures, we summarize and discuss results on the properties of and relationships between these measures, and point to open research problems. We hope our work lays a foundation for more systematic research on the properties and applicability of similarity measures for neural network models.},
	language = {en},
	number = {9},
	urldate = {2025-05-19},
	journal = {ACM Computing Surveys},
	author = {Klabunde, Max and Schumacher, Tobias and Strohmaier, Markus and Lemmerich, Florian},
	month = sep,
	year = {2025},
	note = {arXiv:2305.06329 [cs]},
	keywords = {Computer Science - Machine Learning},
	pages = {1--52},
	annote = {Comment: ACM Computing Surveys},
	file = {PDF:files/93/Klabunde et al. - 2025 - Similarity of Neural Network Models A Survey of Functional and Representational Measures.pdf:application/pdf},
}

@misc{kornblith_similarity_2019-1,
	title = {Similarity of {Neural} {Network} {Representations} {Revisited}},
	url = {http://arxiv.org/abs/1905.00414},
	doi = {10.48550/arXiv.1905.00414},
	abstract = {Recent work has sought to understand the behavior of neural networks by comparing representations between layers and between different trained models. We examine methods for comparing neural network representations based on canonical correlation analysis (CCA). We show that CCA belongs to a family of statistics for measuring multivariate similarity, but that neither CCA nor any other statistic that is invariant to invertible linear transformation can measure meaningful similarities between representations of higher dimension than the number of data points. We introduce a similarity index that measures the relationship between representational similarity matrices and does not suffer from this limitation. This similarity index is equivalent to centered kernel alignment (CKA) and is also closely connected to CCA. Unlike CCA, CKA can reliably identify correspondences between representations in networks trained from different initializations.},
	language = {en},
	urldate = {2025-05-20},
	publisher = {arXiv},
	author = {Kornblith, Simon and Norouzi, Mohammad and Lee, Honglak and Hinton, Geoffrey},
	month = jul,
	year = {2019},
	note = {arXiv:1905.00414 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Quantitative Biology - Neurons and Cognition},
	annote = {Comment: ICML 2019},
	file = {PDF:files/97/Kornblith et al. - 2019 - Similarity of Neural Network Representations Revisited.pdf:application/pdf},
}

@article{csiszarik_similarity_nodate,
	title = {Similarity and {Matching} of {Neural} {Network} {Representations}},
	abstract = {We employ a toolset — dubbed Dr. Frankenstein — to analyse the similarity of representations in deep neural networks. With this toolset, we aim to match the activations on given layers of two trained neural networks by joining them with a stitching layer. We demonstrate that the inner representations emerging in deep convolutional neural networks with the same architecture but different initializations can be matched with a surprisingly high degree of accuracy even with a single, afﬁne stitching layer. We choose the stitching layer from several possible classes of linear transformations and investigate their performance and properties. The task of matching representations is closely related to notions of similarity. Using this toolset, we also provide a novel viewpoint on the current line of research regarding similarity indices of neural network representations: the perspective of the performance on a task.},
	language = {en},
	author = {Csiszárik, Adrián and Ko, Péter and Papp, Gergely and Varga, Dániel},
	file = {PDF:files/100/Csiszárik et al. - Similarity and Matching of Neural Network Representations.pdf:application/pdf},
}

@article{ashmore_evaluating_nodate,
	title = {Evaluating the {Intrinsic} {Similarity} between {Neural} {Networks}},
	language = {en},
	author = {Ashmore, Stephen Charles},
	file = {PDF:files/102/Ashmore - Evaluating the Intrinsic Similarity between Neural Networks.pdf:application/pdf},
}

@article{morcos_insights_2018,
	title = {Insights on representational similarity in neural networks with canonical correlation},
	volume = {31},
	url = {https://proceedings.neurips.cc/paper/2018/hash/a7a3d70c6d17a73140918996d03c014f-Abstract.html},
	language = {en},
	urldate = {2025-05-20},
	journal = {Advances in Neural Information Processing Systems},
	author = {Morcos, Ari and Raghu, Maithra and Bengio, Samy},
	year = {2018},
	file = {Full Text PDF:files/105/Morcos et al. - 2018 - Insights on representational similarity in neural networks with canonical correlation.pdf:application/pdf},
}

@article{morcos_insights_nodate,
	title = {Insights on representational similarity in neural networks with canonical correlation},
	abstract = {Comparing different neural network representations and determining how representations evolve over time remain challenging open questions in our understanding of the function of neural networks. Comparing representations in neural networks is fundamentally difﬁcult as the structure of representations varies greatly, even across groups of networks trained on identical tasks, and over the course of training. Here, we develop projection weighted CCA (Canonical Correlation Analysis) as a tool for understanding neural networks, building off of SVCCA, a recently proposed method [22]. We ﬁrst improve the core method, showing how to differentiate between signal and noise, and then apply this technique to compare across a group of CNNs, demonstrating that networks which generalize converge to more similar representations than networks which memorize, that wider networks converge to more similar solutions than narrow networks, and that trained networks with identical topology but different learning rates converge to distinct clusters with diverse representations. We also investigate the representational dynamics of RNNs, across both training and sequential timesteps, ﬁnding that RNNs converge in a bottom-up pattern over the course of training and that the hidden state is highly variable over the course of a sequence, even when accounting for linear transforms. Together, these results provide new insights into the function of CNNs and RNNs, and demonstrate the utility of using CCA to understand representations.},
	language = {en},
	author = {Morcos, Ari and Raghu, Maithra and Bengio, Samy},
	file = {PDF:files/106/Morcos et al. - Insights on representational similarity in neural networks with canonical correlation.pdf:application/pdf},
}

@misc{noauthor_generalized_nodate,
	title = {Generalized {Shape} {Metrics} on {Neural} {Representations}},
	url = {https://proceedings.neurips.cc/paper/2021/hash/252a3dbaeb32e7690242ad3b556e626b-Abstract.html},
	urldate = {2025-05-21},
	file = {Generalized Shape Metrics on Neural Representations:files/109/252a3dbaeb32e7690242ad3b556e626b-Abstract.html:text/html},
}

@inproceedings{williams_generalized_2021-1,
	title = {Generalized {Shape} {Metrics} on {Neural} {Representations}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/252a3dbaeb32e7690242ad3b556e626b-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Williams, Alex H and Kunz, Erin and Kornblith, Simon and Linderman, Scott},
	editor = {Ranzato, M. and Beygelzimer, A. and Dauphin, Y. and Liang, P. S. and Vaughan, J. Wortman},
	year = {2021},
	pages = {4738--4750},
}

@misc{chan_affine_2024-1,
	title = {On {Affine} {Homotopy} between {Language} {Encoders}},
	url = {http://arxiv.org/abs/2406.02329},
	doi = {10.48550/arXiv.2406.02329},
	abstract = {Pre-trained language encoders—functions that represent text as vectors—are an integral component of many NLP tasks. We tackle a natural question in language encoder analysis: What does it mean for two encoders to be similar? We contend that a faithful measure of similarity needs to be intrinsic, that is, task-independent, yet still be informative of extrinsic similarity—the performance on downstream tasks. It is common to consider two encoders similar if they are homotopic, i.e., if they can be aligned through some transformation.1 In this spirit, we study the properties of affine alignment of language encoders and its implications on extrinsic similarity. We find that while affine alignment is fundamentally an asymmetric notion of similarity, it is still informative of extrinsic similarity. We confirm this on datasets of natural language representations. Beyond providing useful bounds on extrinsic similarity, affine intrinsic similarity also allows us to begin uncovering the structure of the space of pre-trained encoders by defining an order over them.},
	language = {en},
	urldate = {2025-05-21},
	publisher = {arXiv},
	author = {Chan, Robin SM and Boumasmoud, Reda and Svete, Anej and Ren, Yuxin and Guo, Qipeng and Jin, Zhijing and Ravfogel, Shauli and Sachan, Mrinmaya and Schölkopf, Bernhard and El-Assady, Mennatallah and Cotterell, Ryan},
	month = dec,
	year = {2024},
	note = {arXiv:2406.02329 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: 10 pages, Accepted at NeurIPS 2024 (Main)},
	file = {PDF:files/111/Chan et al. - 2024 - On Affine Homotopy between Language Encoders.pdf:application/pdf},
}

@inproceedings{raghu_svcca_2017,
	title = {{SVCCA}: {Singular} {Vector} {Canonical} {Correlation} {Analysis} for {Deep} {Learning} {Dynamics} and {Interpretability}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Raghu, Maithra and Gilmer, Justin and Yosinski, Jason and Sohl-Dickstein, Jascha},
	editor = {Guyon, I. and Luxburg, U. Von and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
	year = {2017},
}

@article{schonemann_generalized_1966,
	title = {A generalized solution of the orthogonal procrustes problem},
	volume = {31},
	issn = {1860-0980},
	url = {https://doi.org/10.1007/BF02289451},
	doi = {10.1007/BF02289451},
	abstract = {A solutionT of the least-squares problemAT=B +E, givenA andB so that trace (E′E)= minimum andT′T=I is presented. It is compared with a less general solution of the same problem which was given by Green [5]. The present solution, in contrast to Green's, is applicable to matricesA andB which are of less than full column rank. Some technical suggestions for the numerical computation ofT and an illustrative example are given.},
	number = {1},
	journal = {Psychometrika},
	author = {Schönemann, Peter H.},
	month = mar,
	year = {1966},
	pages = {1--10},
}

@inproceedings{cristianini_kernel-target_2001,
	title = {On {Kernel}-{Target} {Alignment}},
	volume = {14},
	url = {https://proceedings.neurips.cc/paper_files/paper/2001/file/1f71e393b3809197ed66df836fe833e5-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {MIT Press},
	author = {Cristianini, Nello and Shawe-Taylor, John and Elisseeff, André and Kandola, Jaz},
	editor = {Dietterich, T. and Becker, S. and Ghahramani, Z.},
	year = {2001},
}

@article{li_convergent_2016,
	title = {{CONVERGENT} {LEARNING}: {DO} {DIFFERENT} {NEURAL} {NETWORKS} {LEARN} {THE} {SAME} {REPRESENTATIONS}?},
	language = {en},
	author = {Li, Yixuan and Yosinski, Jason and Clune, Jeff and Lipson, Hod and Hopcroft, John},
	year = {2016},
	file = {PDF:files/117/Li et al. - 2016 - CONVERGENT LEARNING DO DIFFERENT NEURAL NETWORKS LEARN THE SAME REPRESENTATIONS.pdf:application/pdf},
}

@misc{tsitsulin_shape_2020,
	title = {The {Shape} of {Data}: {Intrinsic} {Distance} for {Data} {Distributions}},
	shorttitle = {The {Shape} of {Data}},
	url = {http://arxiv.org/abs/1905.11141},
	doi = {10.48550/arXiv.1905.11141},
	abstract = {The ability to represent and compare machine learning models is crucial in order to quantify subtle model changes, evaluate generative models, and gather insights on neural network architectures. Existing techniques for comparing data distributions focus on global data properties such as mean and covariance; in that sense, they are extrinsic and uni-scale. We develop a ﬁrst-of-its-kind intrinsic and multi-scale method for characterizing and comparing data manifolds, using a lower-bound of the spectral Gromov-Wasserstein inter-manifold distance, which compares all data moments. In a thorough experimental study, we demonstrate that our method effectively discerns the structure of data manifolds even on unaligned data of different dimensionality, and showcase its efﬁcacy in evaluating the quality of generative models.},
	language = {en},
	urldate = {2025-05-22},
	publisher = {arXiv},
	author = {Tsitsulin, Anton and Munkhoeva, Marina and Mottin, Davide and Karras, Panagiotis and Bronstein, Alex and Oseledets, Ivan and Müller, Emmanuel},
	month = feb,
	year = {2020},
	note = {arXiv:1905.11141 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Published in ICLR'2020},
	file = {PDF:files/119/Tsitsulin et al. - 2020 - The Shape of Data Intrinsic Distance for Data Distributions.pdf:application/pdf},
}

@misc{wang_towards_2018,
	title = {Towards {Understanding} {Learning} {Representations}: {To} {What} {Extent} {Do} {Different} {Neural} {Networks} {Learn} the {Same} {Representation}},
	shorttitle = {Towards {Understanding} {Learning} {Representations}},
	url = {http://arxiv.org/abs/1810.11750},
	doi = {10.48550/arXiv.1810.11750},
	abstract = {It is widely believed that learning good representations is one of the main reasons for the success of deep neural networks. Although highly intuitive, there is a lack of theory and systematic approach quantitatively characterizing what representations do deep neural networks learn. In this work, we move a tiny step towards a theory and better understanding of the representations. Specifically, we study a simpler problem: How similar are the representations learned by two networks with identical architecture but trained from different initializations. We develop a rigorous theory based on the neuron activation subspace match model. The theory gives a complete characterization of the structure of neuron activation subspace matches, where the core concepts are maximum match and simple match which describe the overall and the finest similarity between sets of neurons in two networks respectively. We also propose efficient algorithms to find the maximum match and simple matches. Finally, we conduct extensive experiments using our algorithms. Experimental results suggest that, surprisingly, representations learned by the same convolutional layers of networks trained from different initializations are not as similar as prevalently expected, at least in terms of subspace match.},
	language = {en},
	urldate = {2025-05-22},
	publisher = {arXiv},
	author = {Wang, Liwei and Hu, Lunjia and Gu, Jiayuan and Wu, Yue and Hu, Zhiqiang and He, Kun and Hopcroft, John},
	month = nov,
	year = {2018},
	note = {arXiv:1810.11750 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 17 pages, 6 figures},
	file = {PDF:files/122/Wang et al. - 2018 - Towards Understanding Learning Representations To What Extent Do Different Neural Networks Learn th.pdf:application/pdf},
}

@misc{nguyen_wide_2021,
	title = {Do {Wide} and {Deep} {Networks} {Learn} the {Same} {Things}? {Uncovering} {How} {Neural} {Network} {Representations} {Vary} with {Width} and {Depth}},
	shorttitle = {Do {Wide} and {Deep} {Networks} {Learn} the {Same} {Things}?},
	url = {http://arxiv.org/abs/2010.15327},
	doi = {10.48550/arXiv.2010.15327},
	abstract = {A key factor in the success of deep neural networks is the ability to scale models to improve performance by varying the architecture depth and width. This simple property of neural network design has resulted in highly effective architectures for a variety of tasks. Nevertheless, there is limited understanding of effects of depth and width on the learned representations. In this paper, we study this fundamental question. We begin by investigating how varying depth and width affects model hidden representations, ﬁnding a characteristic block structure in the hidden representations of larger capacity (wider or deeper) models. We demonstrate that this block structure arises when model capacity is large relative to the size of the training set, and is indicative of the underlying layers preserving and propagating the dominant principal component of their representations. This discovery has important ramiﬁcations for features learned by different models, namely, representations outside the block structure are often similar across architectures with varying widths and depths, but the block structure is unique to each model. We analyze the output predictions of different model architectures, ﬁnding that even when the overall accuracy is similar, wide and deep models exhibit distinctive error patterns and variations across classes.},
	language = {en},
	urldate = {2025-05-22},
	publisher = {arXiv},
	author = {Nguyen, Thao and Raghu, Maithra and Kornblith, Simon},
	month = apr,
	year = {2021},
	note = {arXiv:2010.15327 [cs]},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: ICLR 2021},
	file = {PDF:files/123/Nguyen et al. - 2021 - Do Wide and Deep Networks Learn the Same Things Uncovering How Neural Network Representations Vary.pdf:application/pdf},
}

@article{nguyen_wide_2021-1,
	title = {{DO} {WIDE} {AND} {DEEP} {NETWORKS} {LEARN} {THE} {SAME} {THINGS}? {UNCOVERING} {HOW} {NEURAL} {NETWORK} {REPRESENTATIONS} {VARY} {WITH} {WIDTH} {AND} {DEPTH}},
	abstract = {A key factor in the success of deep neural networks is the ability to scale models to improve performance by varying the architecture depth and width. This simple property of neural network design has resulted in highly effective architectures for a variety of tasks. Nevertheless, there is limited understanding of effects of depth and width on the learned representations. In this paper, we study this fundamental question. We begin by investigating how varying depth and width affects model hidden representations, ﬁnding a characteristic block structure in the hidden representations of larger capacity (wider or deeper) models. We demonstrate that this block structure arises when model capacity is large relative to the size of the training set, and is indicative of the underlying layers preserving and propagating the dominant principal component of their representations. This discovery has important ramiﬁcations for features learned by different models, namely, representations outside the block structure are often similar across architectures with varying widths and depths, but the block structure is unique to each model. We analyze the output predictions of different model architectures, ﬁnding that even when the overall accuracy is similar, wide and deep models exhibit distinctive error patterns and variations across classes.},
	language = {en},
	author = {Nguyen, Thao and Raghu, Maithra and Kornblith, Simon},
	year = {2021},
	file = {PDF:files/128/Nguyen et al. - 2021 - DO WIDE AND DEEP NETWORKS LEARN THE SAME THINGS UNCOVERING HOW NEURAL NETWORK REPRESENTATIONS VARY.pdf:application/pdf},
}

@inproceedings{sharma_conceptual_2018,
	address = {Melbourne, Australia},
	title = {Conceptual {Captions}: {A} {Cleaned}, {Hypernymed}, {Image} {Alt}-text {Dataset} {For} {Automatic} {Image} {Captioning}},
	shorttitle = {Conceptual {Captions}},
	url = {http://aclweb.org/anthology/P18-1238},
	doi = {10.18653/v1/P18-1238},
	abstract = {We present a new dataset of image caption annotations, Conceptual Captions, which contains an order of magnitude more images than the MS-COCO dataset (Lin et al., 2014) and represents a wider variety of both images and image caption styles. We achieve this by extracting and ﬁltering image caption annotations from billions of webpages. We also present quantitative evaluations of a number of image captioning models and show that a model architecture based on Inception-ResNetv2 (Szegedy et al., 2016) for image-feature extraction and Transformer (Vaswani et al., 2017) for sequence modeling achieves the best performance when trained on the Conceptual Captions dataset.},
	language = {en},
	urldate = {2025-05-22},
	booktitle = {Proceedings of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Sharma, Piyush and Ding, Nan and Goodman, Sebastian and Soricut, Radu},
	year = {2018},
	pages = {2556--2565},
	file = {PDF:files/130/Sharma et al. - 2018 - Conceptual Captions A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning.pdf:application/pdf},
}

@article{goubault-larrecq_non-hausdorff_nodate,
	title = {Non-{Hausdorff} {Topology} and {Domain} {Theory}},
	language = {en},
	author = {Goubault-Larrecq, Jean},
	file = {PDF:files/132/Goubault-Larrecq - Non-Hausdorff Topology and Domain Theory.pdf:application/pdf},
}

@book{sammut_encyclopedia_2017,
	address = {Boston, MA},
	title = {Encyclopedia of {Machine} {Learning} and {Data} {Mining}},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-1-4899-7685-7 978-1-4899-7687-1},
	url = {http://link.springer.com/10.1007/978-1-4899-7687-1},
	language = {en},
	urldate = {2025-05-31},
	publisher = {Springer US},
	editor = {Sammut, Claude and Webb, Geoffrey I.},
	year = {2017},
	doi = {10.1007/978-1-4899-7687-1},
	file = {PDF:files/134/Sammut und Webb - 2017 - Encyclopedia of Machine Learning and Data Mining.pdf:application/pdf},
}

@misc{beck_computationally_2023-1,
	title = {On a {Computationally} {Ill}-{Behaved} {Bilevel} {Problem} with a {Continuous} and {Nonconvex} {Lower} {Level}},
	url = {http://arxiv.org/abs/2202.01033},
	doi = {10.48550/arXiv.2202.01033},
	abstract = {It is well known that bilevel optimization problems are hard to solve both in theory and practice. In this paper, we highlight a further computational diﬃculty when it comes to solving bilevel problems with continuous but nonconvex lower levels. Even if the lower-level problem is solved to ε-feasibility regarding its nonlinear constraints for an arbitrarily small but positive ε, the obtained bilevel solution as well as its objective value may be arbitrarily far away from the actual bilevel solution and its actual objective value. This result even holds for bilevel problems for which the nonconvex lower level is uniquely solvable, for which the strict complementarity condition holds, for which the feasible set is convex, and for which Slater’s constraint qualiﬁcation is satisﬁed for all feasible upper-level decisions. Since the consideration of ε-feasibility cannot be avoided when solving nonconvex problems to global optimality, our result shows that computational bilevel optimization with continuous and nonconvex lower levels needs to be done with great care. Finally, we illustrate that the nonlinearities in the lower level are the key reason for the observed bad behavior by showing that linear bilevel problems behave much better at least on the level of feasible solutions.},
	language = {en},
	urldate = {2025-06-04},
	publisher = {arXiv},
	author = {Beck, Yasmine and Bienstock, Daniel and Schmidt, Martin and Thürauf, Johannes},
	month = may,
	year = {2023},
	note = {arXiv:2202.01033 [math]},
	keywords = {Mathematics - Optimization and Control},
	file = {PDF:files/136/Beck et al. - 2023 - On a Computationally Ill-Behaved Bilevel Problem with a Continuous and Nonconvex Lower Level.pdf:application/pdf},
}

@article{gong_ex2vec_2025,
	title = {{Ex2Vec}: {Enhancing} assembly code semantics with end-to-end execution-aware embeddings},
	volume = {189},
	issn = {0893-6080},
	shorttitle = {{Ex2Vec}},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608025003855},
	doi = {10.1016/j.neunet.2025.107506},
	abstract = {Binary code similarity detection (BSCD), whose goal is to identify and analyze similar or identical functions in compiled binaries, is an essential task in computer security. Recent methods leveraging deep neural networks (DNN) for numerical vector representation of code have achieved significant success. However, these methods primarily adapt techniques from masked language modeling (MLM), encoding code instructions by predicting missing values from an instruction context, which limits their ability to fully capture execution semantics. In this paper, we propose Ex2vec, an innovative end-to-end encoding method that generates high-quality embeddings rich in execution semantics for BCSD. Ex2vec employs a novel pre-training strategy that enables the model to learn the impact of assembly instructions on register states, thus mitigating the reliance on learning the frequency and co-occurrence of the instructions in the assembly context. By simulating the execution of assembly instructions, Ex2Vec accurately captures the semantic features of assembly code, which is further demonstrated by Principal Component Analysis (PCA) that functionally similar instructions cluster closely in the embedding space. Extensive experiments on large datasets validate that Ex2vec performs exceptionally well in binary code similarity detection, surpassing all existing state-of-the-art methods. In real-world vulnerability detection experiments, Ex2Vec exhibits the highest accuracy.},
	urldate = {2025-06-04},
	journal = {Neural Networks},
	author = {Gong, Xingyu and Xu, Yang and Zhang, Sicong and He, Chenhang},
	month = sep,
	year = {2025},
	keywords = {Binary code similarity detection, Binary similarity analysis, Function semantic, Graph Matching Networks (GMN), Transformer, Vulnerability detection},
	pages = {107506},
	file = {ScienceDirect Snapshot:files/139/S0893608025003855.html:text/html},
}

@misc{noauthor_httpsarxivorgpdf250421803_nodate,
	title = {https://arxiv.org/pdf/2504.21803},
	url = {https://arxiv.org/pdf/2504.21803},
	urldate = {2025-06-04},
}

@misc{shang_empirical_2025,
	title = {An {Empirical} {Study} on the {Effectiveness} of {Large} {Language} {Models} for {Binary} {Code} {Understanding}},
	url = {http://arxiv.org/abs/2504.21803},
	doi = {10.48550/arXiv.2504.21803},
	abstract = {Binary code analysis plays a pivotal role in the field of software security and is widely used in tasks such as software maintenance, malware detection, software vulnerability discovery, patch analysis, etc. However, unlike source code, reverse engineers face significant challenges in understanding binary code due to the lack of intuitive semantic information. Although traditional reverse tools can convert binary code into C-like pseudo code, the lack of code comments and symbolic information such as function names still makes code understanding difficult. In recent years, two groups of techniques have shown promising prospects: (1) Deep learning-based techniques have demonstrated competitive results in tasks related to binary code understanding, furthermore, (2) Large Language Models (LLMs) have been extensively pre-trained at the source-code level for tasks such as code understanding and generation. This has left participants wondering about the capabilities of LLMs in binary code understanding. To this end, this work proposes a benchmark to evaluate the effectiveness of LLMs in real-world reverse engineering scenarios, which covers two key binary code understanding tasks, i.e., function name recovery and binary code summarization. To more comprehensively evaluate, we include binaries with multiple target architectures as well as different optimization options. We gain valuable insights into the capabilities and limitations through extensive empirical studies of popular LLMs using our benchmark. Our evaluations reveal that existing LLMs can understand binary code to a certain extent, thereby improving the efficiency of binary code analysis. Our results highlight the great potential of the LLMs in advancing the field of binary code understanding, and provide new directions for binary code analysis techniques.},
	language = {en},
	urldate = {2025-06-04},
	publisher = {arXiv},
	author = {Shang, Xiuwei and Fu, Zhenkan and Cheng, Shaoyin and Chen, Guoqiang and Li, Gangyang and Hu, Li and Zhang, Weiming and Yu, Nenghai},
	month = apr,
	year = {2025},
	note = {arXiv:2504.21803 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Software Engineering},
	annote = {Comment: 38 pages, 9 figures},
	file = {PDF:files/140/Shang et al. - 2025 - An Empirical Study on the Effectiveness of Large Language Models for Binary Code Understanding.pdf:application/pdf},
}

@misc{goodfellow_generative_2014,
	title = {Generative {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1406.2661},
	doi = {10.48550/arXiv.1406.2661},
	abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
	language = {en},
	urldate = {2025-06-06},
	publisher = {arXiv},
	author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	month = jun,
	year = {2014},
	note = {arXiv:1406.2661 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {PDF:files/144/Goodfellow et al. - 2014 - Generative Adversarial Networks.pdf:application/pdf},
}

@misc{noauthor_ma_entwurf_zwei_nodate,
	title = {{MA}\_Entwurf\_Zwei},
	url = {https://de.overleaf.com/project/67935971f44ef5ebc2dd1b88},
	abstract = {Ein einfach bedienbarer Online-LaTeX-Editor. Keine Installation notwendig, Zusammenarbeit in Echtzeit, Versionskontrolle, Hunderte von LaTeX-Vorlagen und mehr},
	language = {de},
	urldate = {2025-06-09},
	file = {Snapshot:files/147/67935971f44ef5ebc2dd1b88.html:text/html},
}

@article{cybenkot_approximation_nodate,
	title = {Approximation by superpositions of a sigmoidal function},
	language = {en},
	author = {Cybenkot, G},
	file = {PDF:files/148/Cybenkot - Approximation by superpositions of a sigmoidal function.pdf:application/pdf},
}

@book{lang_algebra_2002,
	address = {New York, NY},
	series = {Graduate {Texts} in {Mathematics}},
	title = {Algebra},
	copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining},
	isbn = {978-1-4612-6551-1 978-1-4613-0041-0},
	url = {https://link.springer.com/10.1007/978-1-4613-0041-0},
	language = {en},
	urldate = {2025-07-12},
	publisher = {Springer},
	author = {Lang, Serge},
	year = {2002},
	doi = {10.1007/978-1-4613-0041-0},
	note = {ISSN: 0072-5285, 2197-5612},
	keywords = {algebra, Algebra Textbook, Category theory, linear algebra, matrix theory, Textbook},
}

@book{werner_funktionalanalysis_2005,
	series = {Springer-{Lehrbuch}},
	title = {Funktionalanalysis},
	isbn = {978-3-540-21381-9},
	url = {https://books.google.de/books?id=MmyQ-c1an7EC},
	publisher = {Springer},
	author = {Werner, D.},
	year = {2005},
}

@book{werner_funktionalanalysis_2005-1,
	series = {Springer-{Lehrbuch}},
	title = {Funktionalanalysis},
	isbn = {978-3-540-21381-9},
	url = {https://books.google.de/books?id=MmyQ-c1an7EC},
	publisher = {Springer},
	author = {Werner, D.},
	year = {2005},
}

@article{cybenko_approximation_1989,
	title = {Approximation by superpositions of a sigmoidal function},
	volume = {2},
	issn = {1435-568X},
	url = {https://doi.org/10.1007/BF02551274},
	doi = {10.1007/BF02551274},
	abstract = {In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.},
	number = {4},
	journal = {Mathematics of Control, Signals and Systems},
	author = {Cybenko, G.},
	month = dec,
	year = {1989},
	pages = {303--314},
}

@article{cybenko_approximation_1989-1,
	title = {Approximation by superpositions of a sigmoidal function},
	volume = {2},
	issn = {1435-568X},
	url = {https://doi.org/10.1007/BF02551274},
	doi = {10.1007/BF02551274},
	abstract = {In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.},
	number = {4},
	journal = {Mathematics of Control, Signals and Systems},
	author = {Cybenko, G.},
	month = dec,
	year = {1989},
	pages = {303--314},
}

@book{knudson_toolkit_2024-1,
	address = {Berlin, Boston},
	title = {A {Toolkit}},
	isbn = {978-3-11-101485-2},
	url = {https://doi.org/10.1515/9783111014852},
	urldate = {2025-07-14},
	publisher = {De Gruyter},
	author = {Knudson, Kevin P.},
	year = {2024},
	doi = {doi:10.1515/9783111014852},
}

@article{clark_electra_2020-1,
	title = {{ELECTRA}: {PRE}-{TRAINING} {TEXT} {ENCODERS} {AS} {DISCRIMINATORS} {RATHER} {THAN} {GENERATORS}},
	abstract = {Masked language modeling (MLM) pre-training methods such as BERT corrupt the input by replacing some tokens with [MASK] and then train a model to reconstruct the original tokens. While they produce good results when transferred to downstream NLP tasks, they generally require large amounts of compute to be effective. As an alternative, we propose a more sample-efﬁcient pre-training task called replaced token detection. Instead of masking the input, our approach corrupts it by replacing some tokens with plausible alternatives sampled from a small generator network. Then, instead of training a model that predicts the original identities of the corrupted tokens, we train a discriminative model that predicts whether each token in the corrupted input was replaced by a generator sample or not. Thorough experiments demonstrate this new pre-training task is more efﬁcient than MLM because the task is deﬁned over all input tokens rather than just the small subset that was masked out. As a result, the contextual representations learned by our approach substantially outperform the ones learned by BERT given the same model size, data, and compute. The gains are particularly strong for small models; for example, we train a model on one GPU for 4 days that outperforms GPT (trained using 30x more compute) on the GLUE natural language understanding benchmark. Our approach also works well at scale, where it performs comparably to RoBERTa and XLNet while using less than 1/4 of their compute and outperforms them when using the same amount of compute.},
	language = {en},
	author = {Clark, Kevin and Luong, Minh-Thang and Le, Quoc V},
	year = {2020},
	file = {PDF:files/157/Clark et al. - 2020 - ELECTRA PRE-TRAINING TEXT ENCODERS AS DISCRIMINATORS RATHER THAN GENERATORS.pdf:application/pdf},
}

@inproceedings{vaswani_attention_2017,
	title = {Attention is {All} you {Need}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Ł ukasz and Polosukhin, Illia},
	editor = {Guyon, I. and Luxburg, U. Von and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
	year = {2017},
}

@article{beck_computationally_2023-2,
	title = {On a {Computationally} {Ill}-{Behaved} {Bilevel} {Problem} with a {Continuous} and {Nonconvex} {Lower} {Level}},
	volume = {198},
	issn = {1573-2878},
	url = {https://doi.org/10.1007/s10957-023-02238-9},
	doi = {10.1007/s10957-023-02238-9},
	abstract = {It is well known that bilevel optimization problems are hard to solve both in theory and practice. In this paper, we highlight a further computational difficulty when it comes to solving bilevel problems with continuous but nonconvex lower levels. Even if the lower-level problem is solved to \$\${\textbackslash}varepsilon \$\$-feasibility regarding its nonlinear constraints for an arbitrarily small but positive \$\${\textbackslash}varepsilon \$\$, the obtained bilevel solution as well as its objective value may be arbitrarily far away from the actual bilevel solution and its actual objective value. This result even holds for bilevel problems for which the nonconvex lower level is uniquely solvable, for which the strict complementarity condition holds, for which the feasible set is convex, and for which Slater’s constraint qualification is satisfied for all feasible upper-level decisions. Since the consideration of \$\${\textbackslash}varepsilon \$\$-feasibility cannot be avoided when solving nonconvex problems to global optimality, our result shows that computational bilevel optimization with continuous and nonconvex lower levels needs to be done with great care. Finally, we illustrate that the nonlinearities in the lower level are the key reason for the observed bad behavior by showing that linear bilevel problems behave much better at least on the level of feasible solutions.},
	number = {1},
	journal = {Journal of Optimization Theory and Applications},
	author = {Beck, Yasmine and Bienstock, Daniel and Schmidt, Martin and Thürauf, Johannes},
	month = jul,
	year = {2023},
	pages = {428--447},
}

@article{raghu_svcca_nodate,
	title = {{SVCCA}: {Singular} {Vector} {Canonical} {Correlation} {Analysis} for {Deep} {Learning} {Dynamics} and {Interpretability}},
	abstract = {We propose a new technique, Singular Vector Canonical Correlation Analysis (SVCCA), a tool for quickly comparing two representations in a way that is both invariant to afﬁne transform (allowing comparison between different layers and networks) and fast to compute (allowing more comparisons to be calculated than with previous methods). We deploy this tool to measure the intrinsic dimensionality of layers, showing in some cases needless over-parameterization; to probe learning dynamics throughout training, ﬁnding that networks converge to ﬁnal representations from the bottom up; to show where class-speciﬁc information in networks is formed; and to suggest new training regimes that simultaneously save computation and overﬁt less.},
	language = {en},
	author = {Raghu, Maithra and Gilmer, Justin and Yosinski, Jason and Sohl-Dickstein, Jascha},
	file = {PDF:files/161/Raghu et al. - SVCCA Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretabilit.pdf:application/pdf},
}

@article{morcos_insights_nodate-1,
	title = {Insights on representational similarity in neural networks with canonical correlation},
	abstract = {Comparing different neural network representations and determining how representations evolve over time remain challenging open questions in our understanding of the function of neural networks. Comparing representations in neural networks is fundamentally difﬁcult as the structure of representations varies greatly, even across groups of networks trained on identical tasks, and over the course of training. Here, we develop projection weighted CCA (Canonical Correlation Analysis) as a tool for understanding neural networks, building off of SVCCA, a recently proposed method [22]. We ﬁrst improve the core method, showing how to differentiate between signal and noise, and then apply this technique to compare across a group of CNNs, demonstrating that networks which generalize converge to more similar representations than networks which memorize, that wider networks converge to more similar solutions than narrow networks, and that trained networks with identical topology but different learning rates converge to distinct clusters with diverse representations. We also investigate the representational dynamics of RNNs, across both training and sequential timesteps, ﬁnding that RNNs converge in a bottom-up pattern over the course of training and that the hidden state is highly variable over the course of a sequence, even when accounting for linear transforms. Together, these results provide new insights into the function of CNNs and RNNs, and demonstrate the utility of using CCA to understand representations.},
	language = {en},
	author = {Morcos, Ari and Raghu, Maithra and Bengio, Samy},
	file = {PDF:files/163/Morcos et al. - Insights on representational similarity in neural networks with canonical correlation.pdf:application/pdf},
}

@article{cortes_algorithms_nodate,
	title = {Algorithms for learning kernels based on centered alignment},
	abstract = {This paper presents new and effective algorithms for learning kernels. In particular, as shown by our empirical results, these algorithms consistently outperform the so-called uniform combination solution that has proven to be difﬁcult to improve upon in the past, as well as other algorithms for learning kernels based on convex combinations of base kernels in both classiﬁcation and regression. Our algorithms are based on the notion of centered alignment which is used as a similarity measure between kernels or kernel matrices. We present a number of novel algorithmic, theoretical, and empirical results for learning kernels based on our notion of centered alignment. In particular, we describe efﬁcient algorithms for learning a maximum alignment kernel by showing that the problem can be reduced to a simple QP and discuss a one-stage algorithm for learning both a kernel and a hypothesis based on that kernel using an alignment-based regularization. Our theoretical results include a novel concentration bound for centered alignment between kernel matrices, the proof of the existence of effective predictors for kernels with high alignment, both for classiﬁcation and for regression, and the proof of stability-based generalization bounds for a broad family of algorithms for learning kernels based on centered alignment. We also report the results of experiments with our centered alignment-based algorithms in both classiﬁcation and regression.},
	language = {en},
	author = {Cortes, Corinna and Mohri, Mehryar and Rostamizadeh, Afshin},
	file = {PDF:files/165/Cortes et al. - Algorithms for learning kernels based on centered alignment.pdf:application/pdf},
}

@article{borji_pros_2019,
	title = {Pros and cons of {GAN} evaluation measures},
	volume = {179},
	issn = {1077-3142},
	url = {https://www.sciencedirect.com/science/article/pii/S1077314218304272},
	doi = {https://doi.org/10.1016/j.cviu.2018.10.009},
	abstract = {Generative models, in particular generative adversarial networks (GANs), have gained significant attention in recent years. A number of GAN variants have been proposed and have been utilized in many applications. Despite large strides in terms of theoretical progress, evaluating and comparing GANs remains a daunting task. While several measures have been introduced, as of yet, there is no consensus as to which measure best captures strengths and limitations of models and should be used for fair model comparison. As in other areas of computer vision and machine learning, it is critical to settle on one or few good measures to steer the progress in this field. In this paper, I review and critically discuss more than 24 quantitative and 5 qualitative measures for evaluating generative models with a particular emphasis on GAN-derived models. I also provide a set of 7 desiderata followed by an evaluation of whether a given measure or a family of measures is compatible with them.},
	journal = {Computer Vision and Image Understanding},
	author = {Borji, Ali},
	year = {2019},
	keywords = {Neural networks, Deep learning, Evaluation, Generative adversarial nets, Generative models},
	pages = {41--65},
}

@article{geuchen_upper_2025,
	title = {Upper and lower bounds for the {Lipschitz} constant of random neural networks},
	volume = {14},
	issn = {2049-8772},
	url = {https://doi.org/10.1093/imaiai/iaaf009},
	doi = {10.1093/imaiai/iaaf009},
	abstract = {Empirical studies have widely demonstrated that neural networks are highly sensitive to small, adversarial perturbations of the input. The worst-case robustness against these so-called adversarial examples can be quantified by the Lipschitz constant of the neural network. In this paper, we study upper and lower bounds for the Lipschitz constant of random ReLU neural networks. Specifically, we assume that the weights and biases follow a generalization of the He initialization, where general symmetric distributions for the biases are permitted. For deep networks of fixed depth and sufficiently large width, our established upper bound is larger than the lower bound by a factor that is logarithmic in the width. In contrast, for shallow neural networks we characterize the Lipschitz constant up to an absolute numerical constant that is independent of all parameters.},
	number = {2},
	journal = {Information and Inference: A Journal of the IMA},
	author = {Geuchen, Paul and Stöger, Dominik and Telaar, Thomas and Voigtlaender, Felix},
	month = apr,
	year = {2025},
	note = {\_eprint: https://academic.oup.com/imaiai/article-pdf/14/2/iaaf009/62938983/iaaf009.pdf},
	pages = {iaaf009},
}

@book{forster_analysis_2025,
	address = {Wiesbaden},
	series = {Grundkurs {Mathematik}},
	title = {Analysis 2: {Differentialrechnung} im ℝⁿ, gewöhnliche {Differentialgleichungen}},
	copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining},
	isbn = {978-3-658-45811-9 978-3-658-45812-6},
	shorttitle = {Analysis 2},
	url = {https://link.springer.com/10.1007/978-3-658-45812-6},
	language = {de},
	urldate = {2025-07-23},
	publisher = {Springer Fachmedien Wiesbaden},
	author = {Forster, Otto and Lindemann, Florian},
	year = {2025},
	doi = {10.1007/978-3-658-45812-6},
	note = {ISSN: 2626-613X, 2626-6148},
	file = {PDF:files/169/Forster und Lindemann - 2025 - Analysis 2 Differentialrechnung im ℝⁿ, gewöhnliche Differentialgleichungen.pdf:application/pdf},
}

@incollection{noauthor_differential-_1968,
	title = {Differential- und {Integralrechnung} einer {Veränderlichen}},
	url = {https://www.degruyter.com/document/doi/10.1515/9783110836684-006/html},
	language = {de},
	urldate = {2025-07-23},
	booktitle = {Handbuch der {Mathematik}},
	publisher = {De Gruyter},
	month = dec,
	year = {1968},
	doi = {10.1515/9783110836684-006},
	pages = {97--161},
	file = {PDF:files/170/1968 - Differential- und Integralrechnung einer Veränderlichen.pdf:application/pdf},
}

@article{beck_notitle_2023,
	volume = {198},
	issn = {1573-2878},
	url = {https://doi.org/10.1007/s10957-023-02238-9},
	doi = {10.1007/s10957-023-02238-9},
	abstract = {It is well known that bilevel optimization problems are hard to solve both in theory and practice. In this paper, we highlight a further computational difficulty when it comes to solving bilevel problems with continuous but nonconvex lower levels. Even if the lower-level problem is solved to \{\vphantom{\}}{\textbackslash}textbackslashvarepsilon {\textbackslash}-feasibility regarding its nonlinear constraints for an arbitrarily small but positive \{\vphantom{\}}{\textbackslash}textbackslashvarepsilon {\textbackslash}, the obtained bilevel solution as well as its objective value may be arbitrarily far away from the actual bilevel solution and its actual objective value. This result even holds for bilevel problems for which the nonconvex lower level is uniquely solvable, for which the strict complementarity condition holds, for which the feasible set is convex, and for which Slater’s constraint qualification is satisfied for all feasible upper-level decisions. Since the consideration of \{\vphantom{\}}{\textbackslash}textbackslashvarepsilon {\textbackslash}-feasibility cannot be avoided when solving nonconvex problems to global optimality, our result shows that computational bilevel optimization with continuous and nonconvex lower levels needs to be done with great care. Finally, we illustrate that the nonlinearities in the lower level are the key reason for the observed bad behavior by showing that linear bilevel problems behave much better at least on the level of feasible solutions.},
	number = {1},
	journal = {Journal of Optimization Theory and Applications},
	author = {Beck, Yasmine and Bienstock, Daniel and Schmidt, Martin and Thürauf, Johannes},
	month = jul,
	year = {2023},
	pages = {428--447},
}
