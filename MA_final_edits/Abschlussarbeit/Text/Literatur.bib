@inproceedings{
chan_affine_2024,
title={On Affine Homotopy between Language Encoders},
author={Robin Chan and Reda Boumasmoud and Anej Svete and Yuxin Ren and Qipeng Guo and Zhijing Jin and Shauli Ravfogel and Mrinmaya Sachan and Bernhard Sch{\"o}lkopf and Mennatallah El-Assady and Ryan Cotterell},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=FTpOwIaWUz}
}


@misc{pedrycz_deep_2020,
	location = {Cham},
    author = {Pedrycz , Witold and Chen, Shyi-Ming},
	title = {Deep Learning: Concepts and Architectures},
	volume = {866},
	rights = {http://www.springer.com/tdm},
	isbn = {978-3-030-31755-3 978-3-030-31756-0},
	url = {http://link.springer.com/10.1007/978-3-030-31756-0},
	series = {Studies in Computational Intelligence},
	shorttitle = {Deep Learning},
	publisher = {Springer International Publishing},
	urldate = {2025-01-09},
	date = {2020},
	langid = {english},
	doi = {10.1007/978-3-030-31756-0},
	file = {PDF:C\:\\Users\\betti\\Zotero\\storage\\4EIXHKY3\\Pedrycz und Chen - 2020 - Deep Learning Concepts and Architectures.pdf:application/pdf},
}


@inproceedings{kornblith_similarity_2019,
	title = {Similarity of Neural Network Representations Revisited},
year = {2019},
	url = {https://proceedings.mlr.press/v97/kornblith19a.html},
	eventtitle = {International Conference on Machine Learning},
	pages = {3519--3529},
	booktitle = {Proceedings of the 36th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Kornblith, Simon and Norouzi, Mohammad and Lee, Honglak and Hinton, Geoffrey},
	urldate = {2025-01-06},
	date = {2019-05-24},
	langid = {english},
	note = {{ISSN}: 2640-3498},
	file = {Full Text PDF:C\:\\Users\\betti\\Zotero\\storage\\AITI8VND\\Kornblith et al. - 2019 - Similarity of Neural Network Representations Revisited.pdf:application/pdf;Supplementary PDF:C\:\\Users\\betti\\Zotero\\storage\\7SE3UEKW\\Kornblith et al. - 2019 - Similarity of Neural Network Representations Revisited.pdf:application/pdf},
}

@article{klabunde_similarity_2024,
author = {Klabunde, Max and Schumacher, Tobias and Strohmaier, Markus and Lemmerich, Florian},
title = {Similarity of Neural Network Models: A Survey of Functional and Representational Measures},
year = {2025},
issue_date = {September 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {9},
issn = {0360-0300},
url = {https://doi.org/10.1145/3728458},
doi = {10.1145/3728458},
abstract = {Measuring similarity of neural networks to understand and improve their behavior has become an issue of great importance and research interest. In this survey, we provide a comprehensive overview of two complementary perspectives of measuring neural network similarity: (i) representational similarity, which considers how activations of intermediate layers differ, and (ii) functional similarity, which considers how models differ in their outputs. In addition to providing detailed descriptions of existing measures, we summarize and discuss results on the properties of and relationships between these measures, and point to open research problems. We hope our work lays a foundation for more systematic research on the properties and applicability of similarity measures for neural network models.},
journal = {ACM Comput. Surv.},
month = may,
articleno = {242},
numpages = {52},
keywords = {Deep learning, representational similarity, functional similarity}
}

@inproceedings{sellam_multiberts_2022,
title={The Multi{BERT}s: {BERT} Reproductions for Robustness Analysis},
author={Thibault Sellam and Steve Yadlowsky and Ian Tenney and Jason Wei and Naomi Saphra and Alexander D'Amour and Tal Linzen and Jasmijn Bastings and Iulia Raluca Turc and Jacob Eisenstein and Dipanjan Das and Ellie Pavlick},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=K0E_F0gFDgA}
}
@inproceedings{clark_electra_2020,
title={ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators},
author={Kevin Clark and Minh-Thang Luong and Quoc V. Le and Christopher D. Manning},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=r1xMH1BtvB}
}




@inproceedings{liu_roberta_2019,
title={Ro{BERT}a: A Robustly Optimized {BERT} Pretraining Approach},
author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
year={2020},
booktitle={International Conference on Learning Representations},
url={https://openreview.net/forum?id=SyxS0T4tvS}
}

@inproceedings{BERT,
 title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
author = "Devlin, Jacob  and
Chang, Ming-Wei  and
Lee, Kenton  and
Toutanova, Kristina",
booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
month = jun,
year = "2019",
address = "Minneapolis, Minnesota",
publisher = "Association for Computational Linguistics",
url = "https://aclanthology.org/N19-1423/",
doi = "10.18653/v1/N19-1423",
pages = "4171--4186",
abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
}

@inproceedings{socher_recursive_2013,
	location = {Seattle, Washington, {USA}},
	title = {Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank},
	url = {https://aclanthology.org/D13-1170/},
	eventtitle = {{EMNLP} 2013},
	pages = {1631--1642},
	booktitle = {Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing},
	publisher = {Association for Computational Linguistics},
	author = {Socher, Richard and Perelygin, Alex and Wu, Jean and Chuang, Jason and Manning, Christopher D. and Ng, Andrew and Potts, Christopher},
	urldate = {2025-01-14},
	date = {2013-10},
	file = {Full Text PDF:C\:\\Users\\betti\\Zotero\\storage\\SUBA6HLX\\Socher et al. - 2013 - Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank.pdf:application/pdf},
}


@inproceedings{dolan_automatically_2005,
	title = {Automatically Constructing a Corpus of Sentential Paraphrases},
	url = {https://aclanthology.org/I05-5002/},
	eventtitle = {{IJCNLP} 2005},
	booktitle = {Proceedings of the Third International Workshop on Paraphrasing ({IWP}2005)},
	author = {Dolan, William B. and Brockett, Chris},
	urldate = {2025-01-14},
	date = {2005},
	file = {Full Text PDF:C\:\\Users\\betti\\Zotero\\storage\\VNDW4TLJ\\Dolan und Brockett - 2005 - Automatically Constructing a Corpus of Sentential Paraphrases.pdf:application/pdf},
}


@book{antiga_deep_2020,
	location = {Shelter Island, {NY}},
	title = {Deep learning with {PyTorch}},
	isbn = {978-1-61729-526-3 978-1-63835-407-9},
    year = {2020},
	pagetotal = {1},
	publisher = {Manning},
	author = {Antiga, Luca and Stevens, Eli and Viehmann, Thomas},
	editoratype = {collaborator},
	date = {2020},
	langid = {english},
	file = {PDF:C\:\\Users\\betti\\Zotero\\storage\\NL57AN8A\\Antiga - 2020 - Deep learning with PyTorch.pdf:application/pdf},
}


@inproceedings{wang_glue_2019,
	title={{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
	author={Alex Wang and Amanpreet Singh and Julian Michael and Felix Hill and Omer Levy and Samuel R. Bowman},
	booktitle={International Conference on Learning Representations},
	year={2019},
	url={https://openreview.net/forum?id=rJ4km2R5t7},
	}



@online{tom_what_is_2024,
        author = {Krantz, Tom and Jonker, Alexandra},
	title = {What Is Data Poisoning? {\textbar} {IBM}},
	url = {https://www.ibm.com/think/topics/data-poisoning},
	shorttitle = {What Is Data Poisoning?},
	abstract = {Data poisoning occurs when threat actors manipulate or corrupt the training data used to develop artificial intelligence ({AI}) and machine learning ({ML}) models.},
	urldate = {2025-01-24},
	date = {2024-12-10},
	langid = {english},
}



@misc{cotterell_formal_2024,
	title = {Formal Aspects of Language Modeling},
	url = {http://arxiv.org/abs/2311.04329},
	doi = {10.48550/arXiv.2311.04329},
	number = {{arXiv}:2311.04329},
	publisher = {{arXiv}},
	author = {Cotterell, Ryan and Svete, Anej and Meister, Clara and Liu, Tianyu and Du, Li},
	urldate = {2025-01-11},
	date = {2024-04-17},
	eprinttype = {arxiv},
	eprint = {2311.04329 [cs]},
	keywords = {Computer Science - Computation and Language},
}
@book{knudson_algtop,
url = {https://doi.org/10.1515/9783111014852},
title = {Algebraic Topology},
subtitle = {A Toolkit},
author = {Kevin P. Knudson},
publisher = {De Gruyter},
address = {Berlin, Boston},
doi = {doi:10.1515/9783111014852},
isbn = {9783111014852},
year = {2024},
lastchecked = {2025-07-14}
}


@inproceedings{ding_grounding_2021,
	title = {Grounding Representation Similarity Through Statistical Testing},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/hash/0c0bf917c7942b5a08df71f9da626f97-Abstract.html},
	pages = {1556--1568},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Ding, Frances and Denain, Jean-Stanislas and Steinhardt, Jacob},
	urldate = {2025-02-11},
	date = {2021},
	file = {Full Text PDF:C\:\\Users\\betti\\Zotero\\storage\\2NVTJY3U\\Ding et al. - 2021 - Grounding Representation Similarity Through Statistical Testing.pdf:application/pdf},
}

@article{shahbazi_riemann_manif,
title = {Using distance on the Riemannian manifold to compare representations in brain and in models},
journal = {NeuroImage},
volume = {239},
pages = {118271},
year = {2021},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2021.118271},
url = {https://www.sciencedirect.com/science/article/pii/S1053811921005474},
author = {Mahdiyar Shahbazi and Ali Shirali and Hamid Aghajan and Hamed Nili}
}


@article{boix_gulp_nodate,
	title = {{GULP}: a prediction-based metric between representations},
	author = {Boix-Adserà, Enric and Lawrence, Hannah and Stepaniants, George and Rigollet, Philippe},
	langid = {english},
	file = {PDF:C\:\\Users\\betti\\Zotero\\storage\\KK266WYG\\Boix-Adserà et al. - GULP a prediction-based metric between representations.pdf:application/pdf},
}

@inproceedings{williams_generalized_2021,
	title = {Generalized Shape Metrics on Neural Representations},
	volume = {34},
        year = {2021},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/hash/252a3dbaeb32e7690242ad3b556e626b-Abstract.html},
	pages = {4738--4750},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Williams, Alex H and Kunz, Erin and Kornblith, Simon and Linderman, Scott},
	urldate = {2025-02-11},
	date = {2021},
	file = {Full Text PDF:C\:\\Users\\betti\\Zotero\\storage\\LQ2GK27Y\\Williams et al. - 2021 - Generalized Shape Metrics on Neural Representations.pdf:application/pdf},
}

@book{small_statistical_2012,
	title = {The Statistical Theory of Shape},
	isbn = {978-1-4612-4032-7},
	url = {https://books.google.ch/books?id=C43bBwAAQBAJ},
	series = {Springer Series in Statistics},
	publisher = {Springer New York},
	author = {Small, C.G.},
	date = {2012},
	lccn = {96013587},
}

@online{wang_survey_dist_metr,
	title = {Survey on distance metric learning and dimensionality reduction in data mining {\textbar} Data Mining and Knowledge Discovery},
    author= {Wei Fang, Sun Jimeng},
	url = {https://link.springer.com/article/10.1007/s10618-014-0356-z},
	urldate = {2025-02-11},
	file = {Survey on distance metric learning and dimensionality reduction in data mining | Data Mining and Knowledge Discovery:C\:\\Users\\betti\\Zotero\\storage\\M4IUGZJL\\s10618-014-0356-z.html:text/html},
}


@article{dasgupta_performance_2005,
	title = {Performance guarantees for hierarchical clustering},
	volume = {70},
	issn = {0022-0000},
	url = {https://www.sciencedirect.com/science/article/pii/S0022000004001321},
	doi = {10.1016/j.jcss.2004.10.006},
	series = {Special Issue on {COLT} 2002},
	pages = {555--569},
	number = {4},
	journaltitle = {Journal of Computer and System Sciences},
	shortjournal = {Journal of Computer and System Sciences},
	author = {Dasgupta, Sanjoy and Long, Philip M.},
	urldate = {2025-02-11},
	date = {2005-06-01},
	keywords = {-Center, Complete linkage, Hierarchical clustering},
	file = {ScienceDirect Snapshot:C\:\\Users\\betti\\Zotero\\storage\\PDL2TSR2\\S0022000004001321.html:text/html},
}

@article{chang_clust_met_space,
author={Chang, Cheng-Shang and Liao, Wanjiun and Chen, Yu-Sheng and Liou, Li-Heng},
journal={ IEEE Transactions on Network Science and Engineering },
title={{ A Mathematical Theory for Clustering in Metric Spaces }},
year={2016},
volume={3},
number={01},
ISSN={2327-4697},
pages={2-16},
abstract={ Clustering is one of the most fundamental problems in data analysis and it has been studied extensively in the literature. Though many clustering algorithms have been proposed, clustering theories that justify the use of these clustering algorithms are still unsatisfactory. In particular, one of the fundamental challenges is to address the following question: What is a cluster in a set of data points? In this paper, we make an attempt to address such a question by considering a set of data points associated with a distance measure (metric). We first propose a new cohesion measure in terms of the distance measure. Using the cohesion measure, we define a cluster as a set of points that are cohesive to themselves. For such a definition, we show there are various equivalent statements that have intuitive explanations. We then consider the second question: How do we find clusters and good partitions of clusters under such a definition? For such a question, we propose a hierarchical agglomerative algorithm and a partitional algorithm. Unlike standard hierarchical agglomerative algorithms, our hierarchical agglomerative algorithm has a specific stopping criterion and it stops with a partition of clusters. Our partitional algorithm, called the $K$ -sets algorithm in the paper, appears to be a new iterative algorithm. Unlike the Lloyd iteration that needs two-step minimization, our $K$ -sets algorithm only takes one-step minimization. One of the most interesting findings of our paper is the duality result between a distance measure and a cohesion measure. Such a duality result leads to a dual $K$ -sets algorithm for clustering a set of data points with a cohesion measure. The dual $K$ -sets algorithm converges in the same way as a sequential version of the classical kernel $K$ -means algorithm. The key difference is that a cohesion measure does not need to be positive semi-definite. },
keywords={Clustering algorithms;Partitioning algorithms;Extraterrestrial measurements;Algorithm design and analysis;Minimization;Kernel},
doi={10.1109/TNSE.2016.2516339},
url = {https://doi.ieeecomputersociety.org/10.1109/TNSE.2016.2516339},
publisher={IEEE Computer Society},
address={Los Alamitos, CA, USA},
month=jan}



@inproceedings{baraty_impact_2011,
	location = {Berlin, Heidelberg},
	title = {The Impact of Triangular Inequality Violations on Medoid-Based Clustering},
	isbn = {978-3-642-21916-0},
	doi = {10.1007/978-3-642-21916-0_31},
	pages = {280--289},
	booktitle = {Foundations of Intelligent Systems},
	publisher = {Springer},
	author = {Baraty, Saaid and Simovici, Dan A. and Zara, Catalin},
	date = {2011},
	langid = {english},
}


@book{zhou_machine_2021,
	location = {Singapore},
        year = {2021},
	title = {Machine Learning},
	rights = {https://www.springer.com/tdm},
	isbn = {978-981-15-1966-6 978-981-15-1967-3},
	url = {https://link.springer.com/10.1007/978-981-15-1967-3},
	publisher = {Springer Singapore},
	author = {Zhou, Zhi-Hua},
	urldate = {2025-02-13},
	date = {2021},
	langid = {english},
	doi = {10.1007/978-981-15-1967-3},
	file = {PDF:C\:\\Users\\betti\\Zotero\\storage\\L483I8UP\\Zhou - 2021 - Machine Learning.pdf:application/pdf},
}


@book{MachLearnFoundation,
  title={Machine Learning: The Basics},
  author={Jung, A.},
  isbn={9789811681929},
  series={Machine Learning: Foundations, Methodologies, and Applications},
  url={https://books.google.de/books?id=Sr62zgEACAAJ},
  year={2022},
  publisher={Springer Nature Singapore}
}

@incollection{DataMining,
    title = {2 - Getting to Know Your Data},
    booktitle = {Data Mining (Third Edition)},
    publisher = {Morgan Kaufmann},
    edition = {Third Edition},
    address = {Boston},
    pages = {39-82},
    year = {2012},
    series = {The Morgan Kaufmann Series in Data Management Systems},
    isbn = {978-0-12-381479-1},
    doi = {10.1016/B978-0-12-381479-1.00002-2},
    author = {Jiawei Han and Micheline Kamber and Jian Pei},
  
}


@article{ReInfLearning,
  title={A comprehensive survey on safe reinforcement learning},
  author={Garc{\i}a, Javier and Fern{\'a}ndez, Fernando},
  journal={Journal of Machine Learning Research},
  volume={16},
  number={1},
  pages={1437--1480},
  year={2015}
}


@book{haralambous_course_2024,
	location = {Cham},
	title = {A Course in Natural Language Processing},
	rights = {https://www.springernature.com/gp/researchers/text-and-data-mining},
	isbn = {978-3-031-27225-7 978-3-031-27226-4},
	url = {https://link.springer.com/10.1007/978-3-031-27226-4},
	publisher = {Springer International Publishing},
	author = {Haralambous, Yannis},
	urldate = {2025-02-13},
	date = {2024},
	langid = {english},
	doi = {10.1007/978-3-031-27226-4},
	file = {PDF:C\:\\Users\\betti\\Zotero\\storage\\SDDTVYDS\\Haralambous - 2024 - A Course in Natural Language Processing.pdf:application/pdf},
}

@book{kamath_deep_2019,
	location = {Cham},
	title = {Deep Learning for {NLP} and Speech Recognition},
	rights = {http://www.springer.com/tdm},
	isbn = {978-3-030-14595-8 978-3-030-14596-5},
	url = {http://link.springer.com/10.1007/978-3-030-14596-5},
	publisher = {Springer International Publishing},
	author = {Kamath, Uday and Liu, John and Whitaker, James},
	urldate = {2025-02-13},
	date = {2019},
	langid = {english},
	doi = {10.1007/978-3-030-14596-5},
	file = {PDF:C\:\\Users\\betti\\Zotero\\storage\\XI43HYG6\\Kamath et al. - 2019 - Deep Learning for NLP and Speech Recognition.pdf:application/pdf},
}


@book{goyal_deep_2018,
	address = {Berkeley, CA},
	title = {Deep {Learning} for {Natural} {Language} {Processing}},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-1-4842-3684-0 978-1-4842-3685-7},
	url = {http://link.springer.com/10.1007/978-1-4842-3685-7},
	language = {en},
	urldate = {2025-05-19},
	publisher = {Apress},
	author = {Goyal, Palash and Pandey, Sumit and Jain, Karan},
	year = {2018},
	doi = {10.1007/978-1-4842-3685-7},
	file = {PDF:C\:\\Users\\bet61969\\Zotero\\storage\\EV6MW6BC\\Goyal et al. - 2018 - Deep Learning for Natural Language Processing.pdf:application/pdf},
}


@inproceedings{vaswani_attention_2023,
	title = {Attention is {All} you {Need}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
	year = {2017},
}


@article{otter_survey_2021,
	title = {A Survey of the Usages of Deep Learning for Natural Language Processing},
	volume = {32},
	issn = {2162-2388},
	url = {https://ieeexplore.ieee.org/document/9075398/?arnumber=9075398},
	doi = {10.1109/TNNLS.2020.2979670},
	pages = {604--624},
	number = {2},
	journaltitle = {{IEEE} Transactions on Neural Networks and Learning Systems},
	author = {Otter, Daniel W. and Medina, Julian R. and Kalita, Jugal K.},
	urldate = {2025-02-13},
	date = {2021-02},
	note = {Conference Name: {IEEE} Transactions on Neural Networks and Learning Systems},
	keywords = {Computational linguistics, Computer architecture, Decoding, deep learning, machine learning, Machine learning, Natural language processing, natural language processing ({NLP}), neural networks, Neural networks, Training},
	file = {Full Text PDF:C\:\\Users\\betti\\Zotero\\storage\\9TC96U9P\\Otter et al. - 2021 - A Survey of the Usages of Deep Learning for Natural Language Processing.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\betti\\Zotero\\storage\\2GEPKQIX\\9075398.html:text/html},
}

@book{dieck_mengentheoretische_top,
    author =  {Dieck, Tammo tom},
    title ={ Mengentheoretische Topologie},
    year = {2009}
}


@article{lawvere_metric_1973,
	title = {Metric spaces, generalized logic, and closed categories},
	volume = {43},
    year = {1973},
	rights = {http://www.springer.com/tdm},
	issn = {0370-7377},
	url = {http://link.springer.com/10.1007/BF02924844},
	doi = {10.1007/BF02924844},
	pages = {135--166},
	number = {1},
	journal = {Rendiconti del Seminario Matematico e Fisico di Milano},
	shortjournal = {Seminario Mat. e. Fis. di Milano},
	author = {Lawvere, F. William},
	urldate = {2025-02-18},
	date = {1973-12},
	langid = {english},
	file = {PDF:C\:\\Users\\betti\\Zotero\\storage\\PNTHIQT6\\Lawvere - 1973 - Metric spaces, generalized logic, and closed categories.pdf:application/pdf},
}
@misc{smets2024mathematicsneuralnetworkslecture,
      title={Mathematics of Neural Networks (Lecture Notes Graduate Course)}, 
      author={Bart M. N. Smets},
      year={2024},
      eprint={2403.04807},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2403.04807}, 
}


@article{nonlin_bilevel_opt,
	title = {On a {Computationally} {Ill}-{Behaved} {Bilevel} {Problem} with a {Continuous} and {Nonconvex} {Lower} {Level}},
volume = {198},
issn = {1573-2878},
url = {https://doi.org/10.1007/s10957-023-02238-9},
doi = {10.1007/s10957-023-02238-9},
abstract = {It is well known that bilevel optimization problems are hard to solve both in theory and practice. In this paper, we highlight a further computational difficulty when it comes to solving bilevel problems with continuous but nonconvex lower levels. Even if the lower-level problem is solved to \$\${\textbackslash}varepsilon \$\$-feasibility regarding its nonlinear constraints for an arbitrarily small but positive \$\${\textbackslash}varepsilon \$\$, the obtained bilevel solution as well as its objective value may be arbitrarily far away from the actual bilevel solution and its actual objective value. This result even holds for bilevel problems for which the nonconvex lower level is uniquely solvable, for which the strict complementarity condition holds, for which the feasible set is convex, and for which Slater’s constraint qualification is satisfied for all feasible upper-level decisions. Since the consideration of \$\${\textbackslash}varepsilon \$\$-feasibility cannot be avoided when solving nonconvex problems to global optimality, our result shows that computational bilevel optimization with continuous and nonconvex lower levels needs to be done with great care. Finally, we illustrate that the nonlinearities in the lower level are the key reason for the observed bad behavior by showing that linear bilevel problems behave much better at least on the level of feasible solutions.},
number = {1},
journal = {Journal of Optimization Theory and Applications},
author = {Beck, Yasmine and Bienstock, Daniel and Schmidt, Martin and Thürauf, Johannes},
month = jul,
year = {2023},
pages = {428--447},
}




@article{lipschitz_estimation,
	title = {Estimation of the {Lipschitz} constant of a function},
	volume = {8},
	copyright = {http://www.springer.com/tdm},
	issn = {0925-5001, 1573-2916},
	url = {http://link.springer.com/10.1007/BF00229304},
	doi = {10.1007/BF00229304},
	abstract = {A number of global optimisation algorithms rely on the value of the Lipschitz constant of the objective function. In this paper we present a stochastic method for estimating the Lipschitz constant. We show that the largest slope in a fixed size sample of slopes has an approximate Reverse Weibull distribution. Such a distribution is fitted to the largest slopes and the location parameter used as an estimator of the Lipschitz constant. Numerical results are presented.},
	language = {en},
	number = {1},
	urldate = {2025-05-07},
	journal = {Journal of Global Optimization},
	author = {Wood, G.R. and Zhang, B.P.},
	month = jan,
	year = {1996},
	file = {PDF:C\:\\Users\\bet61969\\Zotero\\storage\\ZUF39AI5\\Wood und Zhang - 1996 - Estimation of the Lipschitz constant of a function.pdf:application/pdf},
}


@inproceedings{peters-etal-2018-deep,
    title = "Deep Contextualized Word Representations",
    author = "Peters, Matthew E.  and
      Neumann, Mark  and
      Iyyer, Mohit  and
      Gardner, Matt  and
      Clark, Christopher  and
      Lee, Kenton  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-1202/",
    doi = "10.18653/v1/N18-1202",
    pages = "2227--2237",
    abstract = "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals."
}



@inproceedings{GPT,
  title={Improving Language Understanding by Generative Pre-Training},
  author={Alec Radford and Karthik Narasimhan},
  year={2018},
  url={https://api.semanticscholar.org/CorpusID:49313245}
}


@InProceedings{li2016convergentlearningdifferentneural,
	title = 	 {Convergent Learning: Do different neural networks learn the same representations?},
	author = 	 {Li, Yixuan and Yosinski, Jason and Clune, Jeff and Lipson, Hod and Hopcroft, John},
	booktitle = 	 {Proceedings of the 1st International Workshop on Feature Extraction: Modern Questions and Challenges at NIPS 2015},
	pages = 	 {196--212},
	year = 	 {2015},
	volume = 	 {44},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {Montreal, Canada},
	month = 	 {11 Dec},
	publisher =    {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v44/li15convergent.pdf},
	url = 	 {https://proceedings.mlr.press/v44/li15convergent.html},
	abstract = 	 {Recent successes in training large, deep neural networks (DNNs) have prompted active investigation into the underlying representations learned on their intermediate layers. Such research is difficult because it requires making sense of non-linear computations performed by millions of learned parameters. However, despite the difficulty, such research is valuable because it increases our ability to understand current models and training algorithms and thus create improved versions of them. We argue for the value of investigating whether neural networks exhibit what we call convergent learning, which is when separately trained DNNs learn features that converge to span similar spaces. We further begin research into this question by introducing two techniques to approximately align neurons from two networks: a bipartite matching approach that makes one-to-one assignments between neurons and a spectral clustering approach that finds many-to-many mappings. Our initial approach to answering this question reveals many interesting, previously unknown properties of neural networks, and we argue that future research into the question of convergent learning will yield many more. The insights described here include (1) that some features are learned reliably in multiple networks, yet other features are not consistently learned; and (2) that units learn to span low-dimensional subspaces and, while these subspaces are common to multiple networks, the specific basis vectors learned are not; (3) that the  average activation values of neurons vary considerably within a network, yet the mean activation values across different networks converge to an almost identical distribution.}
}

@inproceedings{tsitsulin2020shapedataintrinsicdistance,
title={The Shape of Data: Intrinsic Distance for Data Distributions},
author={Anton Tsitsulin and Marina Munkhoeva and Davide Mottin and Panagiotis Karras and Alex Bronstein and Ivan Oseledets and Emmanuel Mueller},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=HyebplHYwB}
}

@article{Hardoon2004CanonicalCA,
  title={Canonical Correlation Analysis: An Overview with Application to Learning Methods},
  author={David Roi Hardoon and S{\'a}ndor Szedm{\'a}k and John Shawe-Taylor},
  journal={Neural Computation},
  year={2004},
  volume={16},
  pages={2639-2664},
  url={https://api.semanticscholar.org/CorpusID:202473}
}

@article{CCA_harold,
 ISSN = {00063444},
 URL = {http://www.jstor.org/stable/2333955},
 author = {Harold Hotelling},
 journal = {Biometrika},
 number = {3/4},
 pages = {321--377},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {Relations Between Two Sets of Variates},
 urldate = {2025-05-21},
 volume = {28},
 year = {1936}
}


@article{raghu2017svccasingularvectorcanonical,
	title = {{SVCCA}: {Singular} {Vector} {Canonical} {Correlation} {Analysis} for {Deep} {Learning} {Dynamics} and {Interpretability}},
	abstract = {We propose a new technique, Singular Vector Canonical Correlation Analysis (SVCCA), a tool for quickly comparing two representations in a way that is both invariant to afﬁne transform (allowing comparison between different layers and networks) and fast to compute (allowing more comparisons to be calculated than with previous methods). We deploy this tool to measure the intrinsic dimensionality of layers, showing in some cases needless over-parameterization; to probe learning dynamics throughout training, ﬁnding that networks converge to ﬁnal representations from the bottom up; to show where class-speciﬁc information in networks is formed; and to suggest new training regimes that simultaneously save computation and overﬁt less.},
	language = {en},
	author = {Raghu, Maithra and Gilmer, Justin and Yosinski, Jason and Sohl-Dickstein, Jascha},
	file = {PDF:C\:\\Users\\bet61969\\Zotero\\storage\\5Q8N66BJ\\Raghu et al. - SVCCA Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretabilit.pdf:application/pdf},
}


@article{morcos2018insightsrepresentationalsimilarityneural,
	title = {Insights on representational similarity in neural networks with canonical correlation},
	abstract = {Comparing different neural network representations and determining how representations evolve over time remain challenging open questions in our understanding of the function of neural networks. Comparing representations in neural networks is fundamentally difﬁcult as the structure of representations varies greatly, even across groups of networks trained on identical tasks, and over the course of training. Here, we develop projection weighted CCA (Canonical Correlation Analysis) as a tool for understanding neural networks, building off of SVCCA, a recently proposed method [22]. We ﬁrst improve the core method, showing how to differentiate between signal and noise, and then apply this technique to compare across a group of CNNs, demonstrating that networks which generalize converge to more similar representations than networks which memorize, that wider networks converge to more similar solutions than narrow networks, and that trained networks with identical topology but different learning rates converge to distinct clusters with diverse representations. We also investigate the representational dynamics of RNNs, across both training and sequential timesteps, ﬁnding that RNNs converge in a bottom-up pattern over the course of training and that the hidden state is highly variable over the course of a sequence, even when accounting for linear transforms. Together, these results provide new insights into the function of CNNs and RNNs, and demonstrate the utility of using CCA to understand representations.},
	language = {en},
	author = {Morcos, Ari and Raghu, Maithra and Bengio, Samy},
	file = {PDF:C\:\\Users\\bet61969\\Zotero\\storage\\QI7JAVP3\\Morcos et al. - Insights on representational similarity in neural networks with canonical correlation.pdf:application/pdf},
}


@inproceedings{ding_NEURIPS2021_0c0bf917,
 author = {Ding, Frances and Denain, Jean-Stanislas and Steinhardt, Jacob},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {1556--1568},
 publisher = {Curran Associates, Inc.},
 title = {Grounding Representation Similarity Through Statistical Testing},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/0c0bf917c7942b5a08df71f9da626f97-Paper.pdf},
 volume = {34},
 year = {2021}
}


@article{schonemann_generalized_1966,
	title = {A generalized solution of the orthogonal procrustes problem},
	volume = {31},
	issn = {1860-0980},
	url = {https://doi.org/10.1007/BF02289451},
	doi = {10.1007/BF02289451},
	abstract = {A solutionT of the least-squares problemAT=B +E, givenA andB so that trace (E′E)= minimum andT′T=I is presented. It is compared with a less general solution of the same problem which was given by Green [5]. The present solution, in contrast to Green's, is applicable to matricesA andB which are of less than full column rank. Some technical suggestions for the numerical computation ofT and an illustrative example are given.},
	number = {1},
	journal = {Psychometrika},
	author = {Schönemann, Peter H.},
	month = mar,
	year = {1966},
	pages = {1--10},
}

@inproceedings{
	bau2018identifyingcontrollingimportantneurons,
title={Representational Dissimilarity Metric Spaces for Stochastic Neural Networks},
author={Lyndon Duong and Jingyang Zhou and Josue Nassar and Jules Berman and Jeroen Olieslagers and Alex H Williams},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=xjb563TH-GH}
}

@inproceedings{duong2023representationaldissimilaritymetricspaces,
	title={Representational Dissimilarity Metric Spaces for Stochastic Neural Networks},
	author={Lyndon Duong and Jingyang Zhou and Josue Nassar and Jules Berman and Jeroen Olieslagers and Alex H Williams},
	booktitle={The Eleventh International Conference on Learning Representations },
	year={2023},
	url={https://openreview.net/forum?id=xjb563TH-GH}
	}

@misc{kingma2022autoencodingvariationalbayes,
      title={Auto-Encoding Variational Bayes}, 
      author={Diederik P Kingma and Max Welling},
      year={2022},
      eprint={1312.6114},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1312.6114}, 
}

@inproceedings{ostrow2023geometrycomparingtemporalstructure,
title={Beyond Geometry: Comparing the Temporal Structure of Computation in Neural Circuits with Dynamical Similarity Analysis},
author={Mitchell Ostrow and Adam Joseph Eisen and Leo Kozachkov and Ila R Fiete},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=7blSUMwe7R}
}


@article{cortes2024algorithmslearningkernelsbased,
	title = {Algorithms for learning kernels based on centered alignment},
	abstract = {This paper presents new and effective algorithms for learning kernels. In particular, as shown by our empirical results, these algorithms consistently outperform the so-called uniform combination solution that has proven to be difﬁcult to improve upon in the past, as well as other algorithms for learning kernels based on convex combinations of base kernels in both classiﬁcation and regression. Our algorithms are based on the notion of centered alignment which is used as a similarity measure between kernels or kernel matrices. We present a number of novel algorithmic, theoretical, and empirical results for learning kernels based on our notion of centered alignment. In particular, we describe efﬁcient algorithms for learning a maximum alignment kernel by showing that the problem can be reduced to a simple QP and discuss a one-stage algorithm for learning both a kernel and a hypothesis based on that kernel using an alignment-based regularization. Our theoretical results include a novel concentration bound for centered alignment between kernel matrices, the proof of the existence of effective predictors for kernels with high alignment, both for classiﬁcation and for regression, and the proof of stability-based generalization bounds for a broad family of algorithms for learning kernels based on centered alignment. We also report the results of experiments with our centered alignment-based algorithms in both classiﬁcation and regression.},
	language = {en},
	author = {Cortes, Corinna and Mohri, Mehryar and Rostamizadeh, Afshin},
	file = {PDF:C\:\\Users\\bet61969\\Zotero\\storage\\N9CF54J8\\Cortes et al. - Algorithms for learning kernels based on centered alignment.pdf:application/pdf},
}

@inproceedings{cristianini_kernel-target_2001,
	title = {On {Kernel}-{Target} {Alignment}},
	volume = {14},
	url = {https://proceedings.neurips.cc/paper_files/paper/2001/file/1f71e393b3809197ed66df836fe833e5-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {MIT Press},
	author = {Cristianini, Nello and Shawe-Taylor, John and Elisseeff, André and Kandola, Jaz},
	year = {2001},
}

@article{csiszarik_similarity_nodate,
	title = {Similarity and {Matching} of {Neural} {Network} {Representations}},
	abstract = {We employ a toolset — dubbed Dr. Frankenstein — to analyse the similarity of representations in deep neural networks. With this toolset, we aim to match the activations on given layers of two trained neural networks by joining them with a stitching layer. We demonstrate that the inner representations emerging in deep convolutional neural networks with the same architecture but different initializations can be matched with a surprisingly high degree of accuracy even with a single, afﬁne stitching layer. We choose the stitching layer from several possible classes of linear transformations and investigate their performance and properties. The task of matching representations is closely related to notions of similarity. Using this toolset, we also provide a novel viewpoint on the current line of research regarding similarity indices of neural network representations: the perspective of the performance on a task.},
	language = {en},
	author = {Csiszárik, Adrián and Ko, Péter and Papp, Gergely and Varga, Dániel},
	file = {PDF:C\:\\Users\\bet61969\\Zotero\\storage\\Q4UJHRHK\\Csiszárik et al. - Similarity and Matching of Neural Network Representations.pdf:application/pdf},
}

@inproceedings{li_convergent_2016,
	title = 	 {Convergent Learning: Do different neural networks learn the same representations?},
	author = 	 {Li, Yixuan and Yosinski, Jason and Clune, Jeff and Lipson, Hod and Hopcroft, John},
	booktitle = 	 {Proceedings of the 1st International Workshop on Feature Extraction: Modern Questions and Challenges at NIPS 2015},
	pages = 	 {196--212},
	year = 	 {2015},
	volume = 	 {44},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {Montreal, Canada},
	month = 	 {11 Dec},
	publisher =    {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v44/li15convergent.pdf},
	url = 	 {https://proceedings.mlr.press/v44/li15convergent.html},
	abstract = 	 {Recent successes in training large, deep neural networks (DNNs) have prompted active investigation into the underlying representations learned on their intermediate layers. Such research is difficult because it requires making sense of non-linear computations performed by millions of learned parameters. However, despite the difficulty, such research is valuable because it increases our ability to understand current models and training algorithms and thus create improved versions of them. We argue for the value of investigating whether neural networks exhibit what we call convergent learning, which is when separately trained DNNs learn features that converge to span similar spaces. We further begin research into this question by introducing two techniques to approximately align neurons from two networks: a bipartite matching approach that makes one-to-one assignments between neurons and a spectral clustering approach that finds many-to-many mappings. Our initial approach to answering this question reveals many interesting, previously unknown properties of neural networks, and we argue that future research into the question of convergent learning will yield many more. The insights described here include (1) that some features are learned reliably in multiple networks, yet other features are not consistently learned; and (2) that units learn to span low-dimensional subspaces and, while these subspaces are common to multiple networks, the specific basis vectors learned are not; (3) that the  average activation values of neurons vary considerably within a network, yet the mean activation values across different networks converge to an almost identical distribution.}
}

@misc{celikyilmaz2021evaluationtextgenerationsurvey,
      title={Evaluation of Text Generation: A Survey}, 
      author={Asli Celikyilmaz and Elizabeth Clark and Jianfeng Gao},
      year={2021},
      eprint={2006.14799},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2006.14799}, 
}


@article{borji2018prosconsganevaluation,
	title = {Pros and cons of {GAN} evaluation measures},
	volume = {179},
	issn = {1077-3142},
	url = {https://www.sciencedirect.com/science/article/pii/S1077314218304272},
	doi = {https://doi.org/10.1016/j.cviu.2018.10.009},
	abstract = {Generative models, in particular generative adversarial networks (GANs), have gained significant attention in recent years. A number of GAN variants have been proposed and have been utilized in many applications. Despite large strides in terms of theoretical progress, evaluating and comparing GANs remains a daunting task. While several measures have been introduced, as of yet, there is no consensus as to which measure best captures strengths and limitations of models and should be used for fair model comparison. As in other areas of computer vision and machine learning, it is critical to settle on one or few good measures to steer the progress in this field. In this paper, I review and critically discuss more than 24 quantitative and 5 qualitative measures for evaluating generative models with a particular emphasis on GAN-derived models. I also provide a set of 7 desiderata followed by an evaluation of whether a given measure or a family of measures is compatible with them.},
	journal = {Computer Vision and Image Understanding},
	author = {Borji, Ali},
	year = {2019},
	keywords = {Deep learning, Evaluation, Generative adversarial nets, Generative models, Neural networks},
	pages = {41--65},
}


@inproceedings{somepalli2022neuralnetslearnmodel,
     author = { Somepalli, Gowthami and Fowl, Liam and Bansal, Arpit and Yeh-Chiang, Ping and Dar, Yehuda and Baraniuk, Richard and Goldblum, Micah and Goldstein, Tom },
     booktitle = { 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) },
     title = {{ Can Neural Nets Learn the Same Model Twice? Investigating Reproducibility and Double Descent from the Decision Boundary Perspective }},
     year = {2022},
     volume = {},
     ISSN = {},
     pages = {13689-13698},
     abstract = { We discuss methods for visualizing neural network decision boundaries and decision regions. We use these visual-izations to investigate issues related to reproducibility and generalization in neural network training. We observe that changes in model architecture (and its associate inductive bias) cause visible changes in decision boundaries, while multiple runs with the same architecture yield results with strong similarities, especially in the case of wide architectures. We also use decision boundary methods to visualize double descent phenomena. We see that decision boundary reproducibility depends strongly on model width. Near the threshold of interpolation, neural network decision bound-aries become fragmented into many small decision regions, and these regions are non-reproducible. Meanwhile, very narrows and very wide networks have high levels of re-producibility in their decision boundaries with relatively few decision regions. We discuss how our observations re-late to the theory of double descent phenomena in convex models. Code is available at https://github.com/somepago/dbViz. },
     keywords = {Training;Interpolation;Computer vision;Computational modeling;Neural networks;Machine learning;Computer architecture},
     doi = {10.1109/CVPR52688.2022.01333},
     url = {https://doi.ieeecomputersociety.org/10.1109/CVPR52688.2022.01333},
     publisher = {IEEE Computer Society},
     address = {Los Alamitos, CA, USA},
     month =Jun}

@inproceedings{bau2017networkdissectionquantifyinginterpretability,
 author={Bau, David and Zhou, Bolei and Khosla, Aditya and Oliva, Aude and Torralba, Antonio},
booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
title={Network Dissection: Quantifying Interpretability of Deep Visual Representations}, 
year={2017},
volume={},
number={},
pages={3319-3327},
keywords={Visualization;Training;Detectors;Image color analysis;Semantics;Image segmentation},
doi={10.1109/CVPR.2017.354}}


@inproceedings{mahendran2014understandingdeepimagerepresentations,
  author={Mahendran, Aravindh and Vedaldi, Andrea},
booktitle={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
title={Understanding deep image representations by inverting them}, 
year={2015},
volume={},
number={},
pages={5188-5196},
keywords={Image reconstruction;Image representation;Visualization;Standards;TV;Neural networks;Noise},
doi={10.1109/CVPR.2015.7299155}}

@misc{wang2022understandingweightsimilarityneural,
      title={Understanding Weight Similarity of Neural Networks via Chain Normalization Rule and Hypothesis-Training-Testing}, 
      author={Guangcong Wang and Guangrun Wang and Wenqi Liang and Jianhuang Lai},
      year={2022},
      eprint={2208.04369},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2208.04369}, 
}

@inproceedings{guth2024universalityneuralencodingscnns,
title={On the universality of neural encodings in {CNN}s},
author={Florentin Guth and Brice M{\'e}nard},
booktitle={ICLR 2024 Workshop on Representational Alignment},
year={2024},
url={https://openreview.net/forum?id=ofEBFOrITI}
}

@inproceedings{sharma_conceptual_2018,
	address = {Melbourne, Australia},
	title = {Conceptual {Captions}: {A} {Cleaned}, {Hypernymed}, {Image} {Alt}-text {Dataset} {For} {Automatic} {Image} {Captioning}},
	shorttitle = {Conceptual {Captions}},
	url = {http://aclweb.org/anthology/P18-1238},
	doi = {10.18653/v1/P18-1238},
	abstract = {We present a new dataset of image caption annotations, Conceptual Captions, which contains an order of magnitude more images than the MS-COCO dataset (Lin et al., 2014) and represents a wider variety of both images and image caption styles. We achieve this by extracting and ﬁltering image caption annotations from billions of webpages. We also present quantitative evaluations of a number of image captioning models and show that a model architecture based on Inception-ResNetv2 (Szegedy et al., 2016) for image-feature extraction and Transformer (Vaswani et al., 2017) for sequence modeling achieves the best performance when trained on the Conceptual Captions dataset.},
	language = {en},
	urldate = {2025-05-22},
	booktitle = {Proceedings of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Sharma, Piyush and Ding, Nan and Goodman, Sebastian and Soricut, Radu},
	year = {2018},
	pages = {2556--2565},
	file = {PDF:C\:\\Users\\bet61969\\Zotero\\storage\\A2EDXA5P\\Sharma et al. - 2018 - Conceptual Captions A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning.pdf:application/pdf},
}

@book{goubault-larrecq_non-hausdorff_nodate,
  author    = {Jean Goubault-Larrecq},
  title     = {Non-Hausdorff Topology and Domain Theory: Selected Topics in Point-Set Topology},
  series    = {New Mathematical Monographs},
  year      = {2013},
  publisher = {Cambridge University Press},
  address   = {Cambridge},
  isbn      = {9781107034136},
  note      = {Available in hardback},

}


@book{sammut_encyclopedia_2017,
	address = {Boston, MA},
	title = {Encyclopedia of {Machine} {Learning} and {Data} {Mining}},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-1-4899-7685-7 978-1-4899-7687-1},
	url = {http://link.springer.com/10.1007/978-1-4899-7687-1},
	language = {en},
	urldate = {2025-05-31},
	publisher = {Springer US},
	year = {2017},
	doi = {10.1007/978-1-4899-7687-1},
	file = {PDF:C\:\\Users\\bet61969\\Zotero\\storage\\D2ZHI6KN\\Sammut und Webb - 2017 - Encyclopedia of Machine Learning and Data Mining.pdf:application/pdf},
}


@article{goodfellow_generative_2014,
author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
title = {Generative adversarial networks},
year = {2020},
issue_date = {November 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {63},
number = {11},
issn = {0001-0782},
url = {https://doi.org/10.1145/3422622},
doi = {10.1145/3422622},
abstract = {Generative adversarial networks are a kind of artificial intelligence algorithm designed to solve the generative modeling problem. The goal of a generative model is to study a collection of training examples and learn the probability distribution that generated them. Generative Adversarial Networks (GANs) are then able to generate more examples from the estimated probability distribution. Generative models based on deep learning are common, but GANs are among the most successful generative models (especially in terms of their ability to generate realistic high-resolution images). GANs have been successfully applied to a wide variety of tasks (mostly in research settings) but continue to present unique challenges and research opportunities because they are based on game theory while most other approaches to generative modeling are based on optimization.},
journal = {Commun. ACM},
month = oct,
pages = {139–144},
numpages = {6}
}


@misc{geuchen_upper_2024,
	title = {Upper and lower bounds for the {Lipschitz} constant of random neural networks},
volume = {14},
issn = {2049-8772},
url = {https://doi.org/10.1093/imaiai/iaaf009},
doi = {10.1093/imaiai/iaaf009},
abstract = {Empirical studies have widely demonstrated that neural networks are highly sensitive to small, adversarial perturbations of the input. The worst-case robustness against these so-called adversarial examples can be quantified by the Lipschitz constant of the neural network. In this paper, we study upper and lower bounds for the Lipschitz constant of random ReLU neural networks. Specifically, we assume that the weights and biases follow a generalization of the He initialization, where general symmetric distributions for the biases are permitted. For deep networks of fixed depth and sufficiently large width, our established upper bound is larger than the lower bound by a factor that is logarithmic in the width. In contrast, for shallow neural networks we characterize the Lipschitz constant up to an absolute numerical constant that is independent of all parameters.},
number = {2},
journal = {Information and Inference: A Journal of the IMA},
author = {Geuchen, Paul and Stöger, Dominik and Telaar, Thomas and Voigtlaender, Felix},
month = apr,
year = {2025},
pages = {iaaf009},
}



@article{cybenkot_approximation_nodate,
	title = {Approximation by superpositions of a sigmoidal function},
	volume = {2},
	issn = {1435-568X},
	url = {https://doi.org/10.1007/BF02551274},
	doi = {10.1007/BF02551274},
	abstract = {In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.},
	number = {4},
	journal = {Mathematics of Control, Signals and Systems},
	author = {Cybenko, G.},
	month = dec,
	year = {1989},
	pages = {303--314},
}

@article{kingma2017adammethodstochasticoptimization,
  title={Adam: A Method for Stochastic Optimization},
author={Diederik P. Kingma and Jimmy Ba},
journal={CoRR},
year={2014},
volume={abs/1412.6980},
url={https://api.semanticscholar.org/CorpusID:6628106}
}

@misc{gokcesu2021generalizedhuberlossrobust,
      title={Generalized Huber Loss for Robust Learning and its Efficient Minimization for a Robust Statistics}, 
      author={Kaan Gokcesu and Hakan Gokcesu},
      year={2021},
      eprint={2108.12627},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2108.12627}, 
}
@ARTICLE{jing_self_supervised_2021,
  author={Jing, Longlong and Tian, Yingli},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Self-Supervised Visual Feature Learning With Deep Neural Networks: A Survey}, 
  year={2021},
  volume={43},
  number={11},
  pages={4037-4058},
  keywords={Task analysis;Visualization;Videos;Training;Learning systems;Feature extraction;Annotations;Self-supervised learning;unsupervised learning;convolutional neural network;transfer learning;deep learning},
  doi={10.1109/TPAMI.2020.2992393}}



@book{lang_algebra_2002,
	address = {New York, NY},
	series = {Graduate {Texts} in {Mathematics}},
	title = {Algebra},
	copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining},
	isbn = {978-1-4612-6551-1 978-1-4613-0041-0},
	url = {https://link.springer.com/10.1007/978-1-4613-0041-0},
	language = {en},
	urldate = {2025-07-12},
	publisher = {Springer},
	author = {Lang, Serge},
	year = {2002},
	doi = {10.1007/978-1-4613-0041-0},
	note = {ISSN: 0072-5285, 2197-5612},
	keywords = {algebra, Algebra Textbook, Category theory, linear algebra, matrix theory, Textbook},
}


@book{werner_funktionalanalysis_2005,
	series = {Springer-{Lehrbuch}},
	title = {Funktionalanalysis},
	isbn = {978-3-540-21381-9},
	url = {https://books.google.de/books?id=MmyQ-c1an7EC},
	publisher = {Springer},
	author = {Werner, D.},
	year = {2005},
}

@book{forster_analysis_2025,
	author    = {Otto Forster and Florian Lindemann},
	title     = {Analysis 2: Differentialrechnung im \(\mathbb{R}^n\), gewöhnliche Differentialgleichungen},
	year      = {2025},
	publisher = {Springer Fachmedien Wiesbaden},
	address   = {Wiesbaden},
	series    = {Grundkurs Mathematik},
	isbn      = {978-3-658-45811-9},
	url       = {https://link.springer.com/10.1007/978-3-658-45812-6},
	doi       = {10.1007/978-3-658-45812-6},
	language  = {de},
	urldate   = {2025-07-23},
	note      = {ISSN 2626-613X, 2626-6148}
}

