In this thesis, we focus on the similarity analysis of language encoders, which are central components of modern \ac{NLP} systems. 
Language encoders transform raw text into structured vector representations that can be processed by machine learning models. 
Since our approach is based on comparing and transforming these encoders, they form a foundational element of this work.

\acf{NLP} enables machines to interpret, process, and generate human language. 
Unlike structured data, natural languages present a wide variety of problems that vary from language to language. 
Structuring or extracting meaningful information from free text represents a great solution, if it is done in the right manner.
Historically, \ac{NLP} relied on rule-based systems that analyzed grammatical structure using syntactic parsers and manually designed algorithms. 
These approaches were often limited in scalability and adaptability across different languages and contexts.
Previously, computer scientists broke a language into its grammatical forms, such as parts of speech, phrases, etc., using complex algorithms. 
With the rise of deep learning, data-driven methods have largely replaced rule-based systems.
Modern \ac{NLP} models can learn complex patterns directly from text corpora and perform tasks such as machine translation, text summarization, question answering, and speech recognition with remarkable accuracy~\cite{goyal_deep_2018}.
This chapter provides a technical introduction to the foundations of \ac{NLP}. Section~\ref{LangMod} introduces the concept of language encoders, which transform text into structured vector representations. 
Section~\ref{TM} then presents transformer-based models, the current state-of-the-art architecture in \ac{NLP}.
%In the end, in section \ref{TMaiA} we present real-word models that are build on the transformer and are used withing this thesis. 
